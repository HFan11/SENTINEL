original
"This paper presents a millimeter-scale cmos 64$\times$64 single charged particle radiation detector system for external beam cancer radiotherapy. A 1$\times$1 $\mu m^2$ diode measures energy deposition by a single charged particle in the depletion region, and the array design provides a large detection area of 512$\times$512 $\mu m^2$. Instead of sensing the voltage drop caused by radiation, the proposed system measures the pulse width, i.e., the time it takes for the voltage to return to its baseline. This obviates the need for using power-hungry and large analog-to-digital converters. A prototype asic is fabricated in tsmc 65 nm lp cmos process and consumes the average static power of 0.535 mw under 1.2 v analog and digital power supply. The functionality of the whole system is successfully verified in a clinical 67.5 mev proton beam setting."
"Routing controllability of autonomous vehicles (avs) has been shown to reduce the impact of selfish routing on network efficiency. However, the assumption that avs would readily allow themselves to be controlled externally by a central agency is unrealistic. In this paper, we propose a joint routing and pricing control scheme that aims to incentivize avs to seek centrally controlled system optimal (so) routing by saving on tolls while user equilibrium (ue) seeking avs and human-driven vehicles (hvs) are subject to a congestion charge. The problem is formulated as a bi-level optimization, in which dynamic tolls are optimized in the upper level, whereas the lower level is a mixed equilibrium simulation-based dynamic traffic assignment model considering mixed fleet of avs and hvs."
"Optomechanics and electromechanics have made it possible to prepare macroscopic mechanical oscillators in their quantum ground states, in quadrature squeezed states, and in entangled states of motion. In addition to coaxing ever larger and more tangible objects into a regime of quantum behavior, this new capability has encouraged ideas of using mechanical oscillators in the processing and communication of quantum information and as precision force sensors operating beyond the standard quantum limit. But the effectively linear interaction between motion and light or electricity precludes access to the broader class of quantum states of motion, such as cat states or energy squeezed states. Indeed, early optomechanical proposals noted the possibility to escape this restriction by creating strong quadratic coupling of motion to light. Although there have been experimental demonstrations of quadratically coupled optomechanical systems, these have not yet accessed nonclassical states of motion."
"Online reviews provide product evaluations for customers to make decisions. Unfortunately, the evaluations can be manipulated using fake reviews (""spams"") by professional spammers, who have learned increasingly insidious and powerful spamming strategies by adapting to the deployed detectors. Spamming strategies are hard to capture, as they can be varying quickly along time, different across spammers and target products, and more critically, remained unknown in most cases. Furthermore, most existing detectors focus on detection accuracy, which is not well-aligned with the goal of maintaining the trustworthiness of product evaluations. To address the challenges, we formulate a minimax game where the spammers and spam detectors compete with each other on their practical goals that are not solely based on detection accuracy. Nash equilibria of the game lead to stable detectors that are agnostic to any mixed detection strategies."
"We developed single-shot shaped pulses for ultra high fidelity (uh-fidelity) population transfer on a 3-level quantum system in lambda configuration. To ensure high fidelity, we use the lewis-riesenfeld (l-r) method to derive a family of solutions leading to an exact transfer, where the solutions follow a single dynamical mode of the l-r invariant. Among this family, we identify a tracking solution with a single parameter to control simultaneously the fidelity of the transfer, the population of the excited state, and robustness. We define a measure of the robustness of an uh-fidelity transfer as the minimum percentile deviation on the pulse areas at which the infidelity rises above $10^{-4}$. The robustness of our shaped pulses is found superior to that of gaussian and adiabatically-optimized pulses for moderate pulse areas."
"In this paper, we study the landscape of the population negative log-likelihood function of gaussian mixture models with a general number of components. Due to nonconvexity, there exist multiple local minima that are not globally optimal, even when the mixture is well-separated. We show that all local minima share the same form of structure that partially identifies the component centers of the true mixture, in the sense that each local minimum involves a non-overlapping combination of fitting multiple gaussians to a single true component and fitting a single gaussian to multiple true components. Our results apply to the setting where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is over-or under-specified. For gaussian mixtures with three components, we obtain sharper results in terms of the scaling with the separation between the components."
"We consider the connection between two constructions of the mirror partner for the calabi-yau orbifold. This orbifold is defined as a quotient by some suitable subgroup $g$ of the phase symmetries of the hypersurface $ x_m $ in the weighted projective space, cut out by a quasi-homogeneous polynomial $w_m$. The first, berglund-h\""ubsch-krawitz (bhk) construction, uses another weighted projective space and the quotient of a new hypersurface $x_{m^t}$ inside it by some dual group $g^t$. In the second, batyrev construction, the mirror partner is constructed as a hypersurface in the toric variety defined by the reflexive polytope dual to the polytope associated with the original calabi-yau orbifold. We give a simple evidence of the equivalence of these two constructions."
"In this work, a generalization of the study of the human gait was made from already existent models in the literature, like models of keller and kockshenev. In this hybrid model, a strategy of metabolic energy minimization is combined in a race process, with a non-linear description of the movement of the mass center's libration, trying to reproduce the behavior of the walk-run transition. The results of the experimental data, for different speed regimes, indicate that the perimeter of the trajectory of the mass center is a relevant quantity in the quantification of this dynamic. An experimental procedure was put into practice in collaboration with the research group in biomedical engineering, basic sciences and laboratories of the manuela beltr\'an university in bogot\'a, colombia."
"We present a new mixed variable symplectic (mvs) integrator for planetary systems, that fully resolve close encounters. The method is based on a time regularisation that allows keeping the stability properties of the symplectic integrators, while also reducing the effective step size whenever two planets encounter. We use a high order mvs scheme such that it is possible to integrate with large time steps far away from close encounters. We show that this algorithm is able to resolve almost exact collisions (i.e with a mutual separation of a fraction of the physical radius) while using the same time-step as in weakly perturbed problem such as the solar system. We demonstrate the long term behaviour on systems of six super-earths experiencing strong scattering for 50 kyr. We compare our algorithm to hybrid methods such as mercury and show that for an equivalent cost we obtain much better energy conservation."
"Entanglement is a key resource for quantum information processing. A widely used tool for detecting entanglement is entanglement witness, where the measurement of the witness operator is guaranteed to be positive for all separable states and can be negative for certain entangled states. In reality, due to the exponentially increasing the hilbert-space dimension with respective to the system size, it is very challenging to construct an efficient entanglement witness for general multipartite entangled states. For $n$-partite greenberger-horne-zeilinger (ghz)-like states, the most robust witness scheme requires $n+1$ local measurement settings and can tolerate up to $1/2$ white noise. As a comparison, the most efficient witness for ghz-like states only needs two local measurement settings and can tolerate up to $1/3$ white noise."
"Within general relativity, the unique stationary solution of an isolated black hole is the kerr spacetime, which has a peculiar multipolar structure depending only on its mass and spin. We develop a general method to extract the multipole moments of arbitrary stationary spacetimes and apply it to a large family of horizonless microstate geometries. The latter can break the axial and equatorial symmetry of the kerr metric and have a much richer multipolar structure, which provides a portal to constrain fuzzball models phenomenologically. We find numerical evidence that all multipole moments are typically larger (in absolute value) than those of a kerr black hole with the same mass and spin. Current measurements of the quadrupole moment of black-hole candidates could place only mild constraints on fuzzballs, while future gravitational-wave detections of extreme mass-ratio inspirals with the space mission lisa will improve these bounds by orders of magnitude."
"Reasoning about graphs evolving over time is a challenging concept in many domains, such as bioinformatics, physics, and social networks. We consider a common case in which edges can be short term interactions (e.g., messaging) or long term structural connections (e.g., friendship). In practice, long term edges are often specified by humans. Human-specified edges can be both expensive to produce and suboptimal for the downstream task. To alleviate these issues, we propose a model based on temporal point processes and variational autoencoders that learns to infer temporal attention between nodes by observing node communication. As temporal attention drives between-node feature propagation, using the dynamics of node interactions to learn this key component provides more flexibility while simultaneously avoiding issues associated with human-specified edges."
"Observations of thermally driven transverse vibration of a photonic crystal waveguide (pcw) are reported. The pcw consists of two parallel nanobeams with a 240 nm vacuum gap between the beams. Models are developed and validated for the transduction of beam motion to phase and amplitude modulation of a weak optical probe propagating in a guided mode (gm) of the pcw for probe frequencies far from and near to the dielectric band edge. Since our pcw has been designed for near-field atom trapping, this research provides a foundation for evaluating possible deleterious effects of thermal motion on optical atomic traps near the surfaces of pcws. Longer term goals are to achieve strong atom-mediated links between individual phonons of vibration and single photons propagating in the gms of the pcw, thereby enabling opto-mechanics at the quantum level with atoms, photons, and phonons."
"We establish natural splittings for the values of global mackey functors at orthogonal, unitary and symplectic groups. In particular, the restriction homomorphisms between the orthogonal, unitary and symplectic groups of adjacent dimensions are naturally split epimorphisms. The interest in the splitting comes from equivariant stable homotopy theory. The equivariant stable homotopy groups of every global spectrum form a global mackey functor, so the splittings imply that certain long exact homotopy group sequences separate into short exact sequences. For the real and complex global thom spectra $\mathbf{mo}$ and $\mathbf{mu}$, the splittings imply the regularity of various euler classes related to the tautological representations of $o(n)$ and $u(n)$."
"Baryonic feedback effects lead to a suppression of the weak lensing angular power spectrum on small scales. The poorly constrained shape and amplitude of this suppression is an important source of uncertainties for upcoming cosmological weak lensing surveys such as euclid or lsst. In this first paper in a series of two, we use simulations to build a euclid-like tomographic mock data-set for the cosmic shear power spectrum and the corresponding covariance matrix, which are both corrected for baryonic effects following the baryonification method of schneider et al. (2019). In addition, we develop an emulator to obtain fast predictions of the baryonic power suppression, allowing us to perform a likelihood inference analysis for a standard $\lambda$cdm cosmology with both cosmological and astrophysical parameters."
"Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages."
"Density-based clustering relies on the idea of linking groups to some specific features of the probability distribution underlying the data. The reference to a true, yet unknown, population structure allows to frame the clustering problem in a standard inferential setting, where the concept of ideal population clustering is defined as the partition induced by the true density function. The nonparametric formulation of this approach, known as modal clustering, draws a correspondence between the groups and the domains of attraction of the density modes. Operationally, a nonparametric density estimate is required and a proper selection of the amount of smoothing, governing the shape of the density and hence possibly the modal structure, is crucial to identify the final partition. In this work, we address the issue of density estimation for modal clustering from an asymptotic perspective."
"The tails of diboson production at the lhc are sensitive to the interference between standard model and higher dimension operators parameterizing the effects of heavy new physics. However, helicity selection rules for the diboson scattering amplitudes set an obstruction to the na\""ive interference contributions of dimension six operators, causing the total diboson rate correction's leading contribution to cancel. In this case, carefully measuring the azimuthal decay angles ""resurrects"" the interference, recouping sensitivity to the ""non-interfering"" operators. We explore these signatures in detail, and find that the eft uncertainties associated with higher-dimensional operators are uniquely well-suppressed by the construction of an asymmetry variable which is only generated by these non-interfering operators, relegating the effects of higher-dimensional, interfering operators to the same status as statistical errors in this observable."
"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science."
"The weak gravity conjecture (wgc) demands the existence of superextremal particles in any consistent quantum theory of gravity. The standard lore is that these particles are introduced to ensure that extremal black holes are either unstable or marginally stable, but it is not clear what is wrong if this doesn't happen. This note shows that, for a generic einstein quantum theory of gravity in ads, exactly stability of extremal black branes is in tension with rigorously proven quantum information theorems about entanglement entropy. Avoiding the contradiction leads to a nonperturbative version of the wgc, which reduces to the usual statement at weak coupling. The argument is general, and it does not rely on either supersymmetry or a particular uv completion, assuming only the validity of einsteinian gravity, effective field theory, and holography."
"Understanding how strongly correlated two-dimensional (2d) systems can give rise to unconventional superconductivity with high critical temperatures is one of the major unsolved problems in condensed matter physics. Ultracold 2d fermi gases have emerged as clean and controllable model systems to study the interplay of strong correlations and reduced dimensionality, but direct evidence of superfluidity in these systems has been missing. Here, we demonstrate superfluidity in an ultracold 2d fermi gas by moving a periodic potential through the system and observing no dissipation below a critical velocity v$_{\rm c}$. We measure v$_{\rm c}$ as a function of interaction strength and find a maximum in the crossover regime between bosonic and fermionic superfluidity. Our measurement establishes ultracold fermi gases as a powerful tool for studying the influence of reduced dimensionality on strongly correlated superfluids."
"Ordinary differential equation (ode) is a mathematical model used in many application areas such as climatology, bioinformatics, and chemical engineering with its intuitive appeal to modeling. Despite ode's wide usage in modeling, frequent absence of their analytic solutions makes it difficult to estimate ode parameters from the data, especially when the model has lots of variables and parameters. This paper proposes a bayesian ode parameter estimating algorithm which is fast and accurate even for models with many parameters. The proposed method approximates an ode model with a state-space model based on equations of a numeric solver. It allows fast estimation by avoiding computations of a whole numerical solution in the likelihood. The posterior is obtained by a variational bayes method, more specifically, the approximate riemannian conjugate gradient method (honkela et al."
"We study the transport properties and superconducting proximity effect in nsn junctions formed by a time-reversal symmetry broken weyl semimetal (wsm) in proximity to an $s$-wave superconductor. We find that the differential conductance and induced pairing amplitudes strongly depend on the angle between the junction direction in real space and the axis separating the weyl nodes in momentum space. We identify the influence of a chiral chemical potential, i.e., the electron population imbalance between weyl nodes of opposite chirality, on the transport characteristics of the junction. Remarkably, we observe a net spin polarization of cooper pairs that are generated via andreev reflection in the two wsm regions. The spin polarization is opposite in the two wsm regions and highly sensitive to the chirality imbalance and excitation energy."
"Metal ion implantation into ceramics has been demonstrated to be an effective and controllable technique for tailoring the surface electrical conductivity of the ceramic piece, and this approach has been used in a number of applications. Importantly, it provides a method for grading the voltage drop across high voltage insulators, and thereby increasing the maximum operational voltage that can be applied across the insulator without surface flashover. However a concern for the use of the method is the long term stability of the implantation-induced conductivity, this especially so if the implanted metal species is readily oxidized. Here we report on our examination of the long-term behavior of the surface conductivity of titanium-implanted alumina. The results indicate that after an initial drop of as much as 40% in the first few weeks after implantation, the conductivity shows only a very slow decrease of about 10% over the following year."
"We introduce a novel skyrme-like conserved current in the effective theory of pions and vector mesons based on the idea of hidden local symmetry. The associated charge is equivalent to the skyrmion charge for any smooth configuration. In addition, there exist singular configurations that can be identified as n_f=1 baryons charged under the new symmetry. Under this identification, the vector mesons play the role of the chern-simons vector fields living on the quantum hall droplet that forms the n_f=1 baryon. We propose that this current is the correct effective expression for the baryon current at low energies. This proposal gives a unified picture for the two types of baryons and allows them to continuously transform one to the other in a natural way. In addition, chern-simons dualities on the droplet can be interpreted as a result of seiberg-like duality between gluons and vector mesons."
"Interfacial spin-orbit coupling in josephson junctions offers an intriguing way to combine anomalous hall and josephson physics in a single device. We study theoretically how the superposition of both effects impacts superconductor/ferromagnetic insulator/superconductor junctions' transport properties. Transverse momentum-dependent skew tunneling of cooper pairs through the spin-active ferromagnetic insulator interface creates sizable transverse hall supercurrents, to which we refer as anomalous josephson hall effect currents. We generalize the furusaki-tsukada formula, which got initially established to quantify usual (tunneling) josephson current flows, to evaluate the transverse current components and demonstrate that their amplitudes are widely adjustable by means of the spin-orbit coupling strengths or the superconducting phase difference across the junction. As a clear spectroscopic fingerprint of josephson junctions, well-localized subgap bound states form around the interface."
"Following the suggestion from the monte--carlo experiments in jim\'enez, j. Of turbul. 2020), that dipoles are as important to the dynamics of decaying two-dimensional turbulence as individual vortex cores, it is found that the kinetic energy of this flow is carried by elongated streams formed by the concatenation of dipoles. Vortices separate into a family of small fast-moving cores, and another family of larger slowly moving ones, which can be described as `frozen' into a slowly evolving `crystal'. The kinematics of both families are very different, and only the former is self-similar. The latter is responsible for most of the kinetic energy of the flow, and its vortices form the dipoles and the streams. Mechanisms are discussed for the growth of this slow component."
"Continuous-time quantum walks have proven to be an extremely useful framework for the design of several quantum algorithms. Often, the running time of quantum algorithms in this framework is characterized by the quantum hitting time: the time required by the quantum walk to find a vertex of interest with a high probability. In this article, we provide improved upper bounds for the quantum hitting time that can be applied to several ctqw-based quantum algorithms. In particular, we apply our techniques to the glued-trees problem, improving their hitting time upper bound by a polynomial factor: from $o(n^5)$ to $o(n^2\log n)$. Furthermore, our methods also help to exponentially improve the dependence on precision of the continuous-time quantum walk based algorithm to find a marked node on any ergodic, reversible markov chain by chakraborty et al."
"Extreme mass ratio inspirals (emris) can be classified as dry emris and wet emris based on their formation mechanisms. Dry (or the ""loss-cone"") emris, previsouly considered as the main emri sources for the laser interferometer space antenna, are primarily produced by multi-body scattering in the nuclear star cluster and gravitational capture. In this letter, we highlight an alternative emri formation channel: (wet) emri formation assisted by the accretion flow around accreting galactic-center massive black holes (mbhs). In this channel, the accretion disk captures stellar-mass black holes that are intially moving on inclined orbits, and subsequently drives them to migrate towards the mbh - this process boosts the formation rate of emris in such galaxies by orders of magnitude."
"When a new user just signs up on a website, we usually have no information about him/her, i.e. No interaction with items, no user profile and no social links with other users. Under such circumstances, we still expect our recommender systems could attract the users at the first time so that the users decide to stay on the website and become active users. This problem falls into new user cold-start category and it is crucial to the development and even survival of a company. Existing works on user cold-start recommendation either require additional user efforts, e.g. Setting up an interview process, or make use of side information [10] such as user demographics, locations, social relations, etc. However, users may not be willing to take the interview and side information on cold-start users is usually not available."
"Deep learning has solved many problems that are out of reach of heuristic algorithms. It has also been successfully applied in wireless communications, even though the current radio systems are well-understood and optimal algorithms exist for many tasks. While some gains have been obtained by learning individual parts of a receiver, a better approach is to jointly learn the whole receiver. This, however, often results in a challenging nonlinear problem, for which the optimal solution is infeasible to implement. To this end, we propose a deep fully convolutional neural network, deeprx, which executes the whole receiver pipeline from frequency domain signal stream to uncoded bits in a 5g-compliant fashion. We facilitate accurate channel estimation by constructing the input of the convolutional neural network in a very specific manner using both the data and pilot symbols."
"Mutation-based fuzzing typically uses an initial set of non-crashing seed inputs (a corpus) from which to generate new inputs by mutation. A corpus of potential seeds will often contain thousands of similar inputs. This lack of diversity can lead to wasted fuzzing effort by exhaustive mutation from all available seeds. To address this, fuzzers come with distillation tools (e.g., afl-cmin) that select the smallest subset of seeds that triggers the same range of instrumentation data points as the full corpus. Common practice suggests that minimizing the number and cumulative size of the seeds leads to more efficient fuzzing, which we explore systematically. We present results of 34+ cpu-years of fuzzing with five distillation approaches to understand their impact in finding bugs in real-world software."
"In this paper we investigate multi-agent discrete-event systems with partial observation. The agents can be divided into several groups in each of which the agents have similar (isomorphic) state transition structures, and thus can be relabeled into the same template. Based on the template a scalable supervisor whose state size and computational cost are independent of the number of agents is designed for the case of partial observation. The scalable supervisor under partial observation does not need to be recomputed regardless of how many agents are added to or removed from the system. We generalize our earlier results to partial observation by proposing sufficient conditions for safety and maximal permissiveness of the scalable least restrictive supervisor on the template level. An example is provided to illustrate the proposed scalable supervisory synthesis."
"Electric network frequency (enf) fluctuations constitute a powerful tool in multimedia forensics. An efficient approach for enf estimation is introduced with temporal windowing based on the filter-bank capon spectral estimator. A type of gohberg-semencul factorization of the model covariance matrix is used due to the toeplitz structure of the covariance matrix. Moreover, this approach uses, for the first time in the field of enf, a temporal window, not necessarily the rectangular one, at the stage preceding spectral estimation. Krylov matrices are employed for fast implementation of matrix inversions. The proposed approach outperforms the state-of-the-art methods in enf estimation, when a short time window of $1$ second is employed in power recordings. In speech recordings, the proposed approach yields highly accurate results with respect to both time complexity and accuracy."
"The direpack package aims to establish a set of modern statistical dimension reduction techniques into the python universe as a single, consistent package. The dimension reduction methods included resort into three categories: projection pursuit based dimension reduction, sufficient dimension reduction, and robust m estimators for dimension reduction. As a corollary, regularized regression estimators based on these reduced dimension spaces are provided as well, ranging from classical principal component regression up to sparse partial robust m regression. The package also contains a set of classical and robust pre-processing utilities, including generalized spatial signs, as well as dedicated plotting functionality and cross-validation utilities. Finally, direpack has been written consistent with the scikit-learn api, such that the estimators can flawlessly be included into (statistical and/or machine) learning pipelines in that framework."
"The nonlinear fluorescence emission has been widely applied for the high spatial resolution optical imaging. Here, we studied the fluorescence anomalous saturating effect of the nitrogen vacancy defect in diamond. The fluorescence reduction was observed with high power laser excitation. It increased the nonlinearity of the fluorescence emission, and changed the spatial frequency distribution of the fluorescence image. We used a differential excitation protocol to extract the high spatial frequency information. By modulating the excitation laser's power, the spatial resolution of imaging was improved approximate 1.6 times in comparison with the confocal microscopy. Due to the simplicity of the experimental setup and data processing, we expect this method can be used for improving the spatial resolution of sensing and biological labeling with the defects in solids."
"When random effects are correlated with sample design variables, the usual approach of employing individual survey weights (constructed to be inversely proportional to the unit survey inclusion probabilities) to form a pseudo-likelihood no longer produces asymptotically unbiased inference. We construct a weight-exponentiated formulation for the random effects distribution that achieves unbiased inference for generating hyperparameters of the random effects. We contrast our approach with frequentist methods that rely on numerical integration to reveal that only the bayesian method achieves both unbiased estimation with respect to the sampling design distribution and consistency with respect to the population generating distribution. Our simulations and real data example for a survey of business establishments demonstrate the utility of our approach across different modeling formulations and sampling designs. This work serves as a capstone for recent developmental efforts that combine traditional survey estimation approaches with the bayesian modeling paradigm and provides a bridge across the two rich but disparate sub-fields."
"Low-symmetry 2d materials---such as res$_2$ and rese$_2$ monolayers, black phosphorus monolayers, group-iv monochalcogenide monolayers, borophene, among others---have more complex atomistic structures than the honeycomb lattices of graphene, hexagonal boron nitride, and transition metal dichalcogenides. The reduced symmetries of these emerging materials give rise to inhomogeneous electron, optical, valley, and spin responses, as well as entirely new properties such as ferroelasticity, ferroelectricity, magnetism, spin-wave phenomena, large nonlinear optical properties, photogalvanic effects, and superconductivity. Novel electronic topological properties, nonlinear elastic properties, and structural phase transformations can also take place due to low symmetry. The ""beyond graphene: low-symmetry and anisotropic 2d materials"" special topic was assembled to highlight recent experimental and theoretical research on these emerging materials."
"Orientational dynamics in the isotropic phase of a comb-shaped nematic polymer with mesogenic and functional side groups was studied using the kerr effect and dielectric spectroscopy. For the first time, it was found that in a mesogenic polymer, in contrast to low-molecular-weight mesogens, the relaxation of the electric birefringence of a melt above the temperature of the nematic-isotropic phase transition can be presented by a sum of several exponential processes, two of which play a decisive role. These main processes replace each other in a temperature range of about fifty degrees. Dielectric spectroscopy also made it possible to distinguish two processes of orientational relaxation: the first is due to rotation of the side mesogenic groups, and the second is associated with motion of the main chain segments."
"Deep hashing methods have shown great retrieval accuracy and efficiency in large-scale image retrieval. How to optimize discrete hash bits is always the focus in deep hashing methods. A common strategy in these methods is to adopt an activation function, e.g. $\operatorname{sigmoid}(\cdot)$ or $\operatorname{tanh}(\cdot)$, and minimize a quantization loss to approximate discrete values. However, this paradigm may make more and more hash bits stuck into the wrong saturated area of the activation functions and never escaped. We call this problem ""dead bits problem~(dbp)"". Besides, the existing quantization loss will aggravate dbp as well. In this paper, we propose a simple but effective gradient amplifier which acts before activation functions to alleviate dbp."
"We study the phenomenology of a hypercharge-zero su(2) triplet scalar whose existence is motivated by two-step electroweak symmetry-breaking. We consider both the possibility that the triplets are stable and contribute to the dark matter density, or that they decay via mixing with the standard model higgs boson. The former is constrained by disappearing charged track searches at the lhc and by dark matter direct detection experiments, while the latter is constrained by existing multilepton collider searches. We find that a two-step electroweak phase transition involving a stable triplet with a negative quadratic term is ruled out by direct detection searches, while an unstable triplet with a mass less than $230\ \mathrm{gev}$ is excluded at $95\%$ confidence level."
"This paper shows that the eccentric debris rings seen around the stars fomalhaut and hd 202628 are narrower than expected in the standard eccentric planet perturbation scenario (sometimes referred to as ""pericenter glow""). The standard scenario posits an initially circular and narrow belt of planetesimals at semi-major axis $a$, whose eccentricity is increased to $e_f$ after the gas disc has dispersed by secular perturbations from an eccentric planet, resulting in a belt of width $2ae_f$. In a minor modification of this scenario, narrower belts can arise if the planetesimals are initially eccentric, which could result from earlier planet perturbations during the gas-rich protoplanetary disc phase. However, a primordial eccentricity could alternatively be caused by instabilities that increase the disc eccentricity, without the need for any planets."
"An electron is usually considered to have only one type of kinetic energy, but could it have more, for its spin and charge, or by exciting other electrons? in one dimension (1d), the physics of interacting electrons is captured well at low energies by the tomonaga-luttinger-liquid (tll) model, yet little has been observed experimentally beyond this linear regime. Here, we report on measurements of many-body modes in 1d gated-wires using a tunnelling spectroscopy technique. We observe two separate fermi seas at high energies, associated with spin and charge excitations, together with the emergence of three additional 1d 'replica' modes that strengthen with decreasing wire length. The effective interaction strength in the wires is varied by changing the amount of 1d inter-subband screening by over 45%."
"$context$. The assembly history experienced by the milky way is currently being unveiled thanks to the data provided by the $gaia$ mission. It is likely that the globular cluster system of our galaxy has followed a similarly intricate formation path. $aims$. To constrain this formation path, we explore the link between the globular clusters and the known merging events that the milky way has experienced. $methods$. To this end, we combined the kinematic information provided by $gaia$ for almost all galactic clusters, with the largest sample of cluster ages available after carefully correcting for systematic errors. To identify clusters with a common origin we analysed their dynamical properties, particularly in the space of integrals of motion. $results$. We find that about 40% of the clusters likely formed in situ."
"The estimation of the intrinsic dimension of a dataset is a fundamental step in most dimensionality reduction techniques. This article illustrates intrinsic, an r package that implements novel state-of-the-art likelihood-based estimators of the intrinsic dimension of a dataset. In detail, the methods included in this package are the two-nn, gride, and hidalgo models. To allow these novel estimators to be easily accessible, the package contains a few high-level, intuitive functions that rely on a broader set of efficient, low-level routines. Intrinsic encompasses models that fall into two categories: homogeneous and heterogeneous intrinsic dimension estimators. The first category contains the two-nn and gride models. The functions dedicated to these two methods carry out inference under both the frequentist and bayesian frameworks. In the second category we find hidalgo, a bayesian mixture model, for which an efficient gibbs sampler is implemented."
"Spintronic nanodevices have ultrafast nonlinear dynamic and recurrence behaviors on a nanosecond scale that promises to enable spintronic reservoir computing (rc) system. Here two physical rc systems based on a single magnetic skyrmion memristor (msm) and 24 spin-torque nano-oscillators (stnos) were proposed and modeled to process image classification task and nonlinear dynamic system prediction, respectively. Based on our micromagnetic simulation results on the nonlinear responses of msm and stno with current pulses stimulation, the handwritten digits recognition task domesticates that an rc system using one single msm has the outstanding performance on image classification. In addition, the complex unknown nonlinear dynamic problems can also be well solved by a physical rc system consisted of 24 stnos confirmed in a second-order nonlinear dynamic system and narma10 tasks."
"Ranking algorithms play a crucial role in online platforms ranging from search engines to recommender systems. In this paper, we identify a surprising consequence of popularity-based rankings: the fewer the items reporting a given signal, the higher the share of the overall traffic they collectively attract. This few-get-richer effect emerges in settings where there are few distinct classes of items (e.g., left-leaning news sources versus right-leaning news sources), and items are ranked based on their popularity. We demonstrate analytically that the few-get-richer effect emerges when people tend to click on top-ranked items and have heterogeneous preferences for the classes of items. Using simulations, we analyze how the strength of the effect changes with assumptions about the setting and human behavior. We also test our predictions experimentally in an online experiment with human participants. Our findings have important implications to understand the spread of misinformation."
"Weight pruning has been widely acknowledged as a straightforward and effective method to eliminate redundancy in deep neural networks (dnn), thereby achieving acceleration on various platforms. However, most of the pruning techniques are essentially trade-offs between model accuracy and regularity which lead to impaired inference accuracy and limited on-device acceleration performance. To solve the problem, we introduce a new sparsity dimension, namely pattern-based sparsity that comprises pattern and connectivity sparsity, and becoming both highly accurate and hardware friendly. With carefully designed patterns, the proposed pruning unprecedentedly and consistently achieves accuracy enhancement and better feature extraction ability on different dnn structures and datasets, and our pattern-aware pruning framework also achieves pattern library extraction, pattern selection, pattern and connectivity pruning and weight training simultaneously. Our approach on the new pattern-based sparsity naturally fits into compiler optimization for highly efficient dnn execution on mobile platforms."
"Difference-in-differences analysis with a control group that differs considerably from a treated group is vulnerable to bias from historical events that have different effects on the groups. Constructing a more closely matched control group by matching a subset of the overall control group to the treated group may result in less bias. We study this phenomenon in simulation studies. We study the effect of mountaintop removal mining (mrm) on mortality using a difference-in-differences analysis that makes use of the increase in mrm following the 1990 clean air act amendments. For a difference-in-differences analysis of the effect of mrm on mortality, we constructed a more closely matched control group and found a 95\% confidence interval that contains substantial adverse effects along with no effect and small beneficial effects."
"When constructing models to summarize clinical data to be used for simulations, it is good practice to evaluate the models for their capacity to reproduce the data. This can be done by means of visual predictive checks (vpc), which consist of (1) several reproductions of the original study by simulation from the model under evaluation, (2) calculating estimates of interest for each simulated study and (3) comparing the distribution of those estimates with the estimate from the original study. This procedure is a generic method that is straightforward to apply, in general. Here we consider the application of the method to time to event data and consider the special case when a time-varying covariate is not known or cannot be approximated after event time."
"Novel concepts, perspectives and challenges in measuring and controlling an open quantum system via sequential schemes are shown. We discuss how similar protocols, relying both on repeated quantum measurements and dynamical decoupling control pulses, can allow to: (i) confine and protect quantum dynamics from decoherence in accordance with the zeno physics. (ii) analytically predict the probability that a quantum system is transferred into a target quantum state by means of stochastic sequential measurements. (iii) optimally reconstruct the spectral density of environmental noise sources by orthogonalizing in the frequency domain the filter functions driving the designed quantum-sensor. The achievement of these tasks will enhance our capability to observe and manipulate open quantum systems, thus bringing advances to quantum science and technologies."
"This paper presents a millimeter-scale cmos 64$\times$64 single charged particle radiation detector system for external beam cancer radiotherapy. A 1$\times$1 $\mu m^2$ diode measures energy deposition by a single charged particle in the depletion region, and the array design provides a large detection area of 512$\times$512 $\mu m^2$. Instead of sensing the voltage drop caused by radiation, the proposed system measures the pulse width, i.e., the time it takes for the voltage to return to its baseline. This obviates the need for using power-hungry and large analog-to-digital converters. A prototype asic is fabricated in tsmc 65 nm lp cmos process and consumes the average static power of 0.535 mw under 1.2 v analog and digital power supply. The functionality of the whole system is successfully verified in a clinical 67.5 mev proton beam setting."
"Routing controllability of autonomous vehicles (avs) has been shown to reduce the impact of selfish routing on network efficiency. However, the assumption that avs would readily allow themselves to be controlled externally by a central agency is unrealistic. In this paper, we propose a joint routing and pricing control scheme that aims to incentivize avs to seek centrally controlled system optimal (so) routing by saving on tolls while user equilibrium (ue) seeking avs and human-driven vehicles (hvs) are subject to a congestion charge. The problem is formulated as a bi-level optimization, in which dynamic tolls are optimized in the upper level, whereas the lower level is a mixed equilibrium simulation-based dynamic traffic assignment model considering mixed fleet of avs and hvs."
"Optomechanics and electromechanics have made it possible to prepare macroscopic mechanical oscillators in their quantum ground states, in quadrature squeezed states, and in entangled states of motion. In addition to coaxing ever larger and more tangible objects into a regime of quantum behavior, this new capability has encouraged ideas of using mechanical oscillators in the processing and communication of quantum information and as precision force sensors operating beyond the standard quantum limit. But the effectively linear interaction between motion and light or electricity precludes access to the broader class of quantum states of motion, such as cat states or energy squeezed states. Indeed, early optomechanical proposals noted the possibility to escape this restriction by creating strong quadratic coupling of motion to light. Although there have been experimental demonstrations of quadratically coupled optomechanical systems, these have not yet accessed nonclassical states of motion."
"Online reviews provide product evaluations for customers to make decisions. Unfortunately, the evaluations can be manipulated using fake reviews (""spams"") by professional spammers, who have learned increasingly insidious and powerful spamming strategies by adapting to the deployed detectors. Spamming strategies are hard to capture, as they can be varying quickly along time, different across spammers and target products, and more critically, remained unknown in most cases. Furthermore, most existing detectors focus on detection accuracy, which is not well-aligned with the goal of maintaining the trustworthiness of product evaluations. To address the challenges, we formulate a minimax game where the spammers and spam detectors compete with each other on their practical goals that are not solely based on detection accuracy. Nash equilibria of the game lead to stable detectors that are agnostic to any mixed detection strategies."
"We developed single-shot shaped pulses for ultra high fidelity (uh-fidelity) population transfer on a 3-level quantum system in lambda configuration. To ensure high fidelity, we use the lewis-riesenfeld (l-r) method to derive a family of solutions leading to an exact transfer, where the solutions follow a single dynamical mode of the l-r invariant. Among this family, we identify a tracking solution with a single parameter to control simultaneously the fidelity of the transfer, the population of the excited state, and robustness. We define a measure of the robustness of an uh-fidelity transfer as the minimum percentile deviation on the pulse areas at which the infidelity rises above $10^{-4}$. The robustness of our shaped pulses is found superior to that of gaussian and adiabatically-optimized pulses for moderate pulse areas."
"In this paper, we study the landscape of the population negative log-likelihood function of gaussian mixture models with a general number of components. Due to nonconvexity, there exist multiple local minima that are not globally optimal, even when the mixture is well-separated. We show that all local minima share the same form of structure that partially identifies the component centers of the true mixture, in the sense that each local minimum involves a non-overlapping combination of fitting multiple gaussians to a single true component and fitting a single gaussian to multiple true components. Our results apply to the setting where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is over-or under-specified. For gaussian mixtures with three components, we obtain sharper results in terms of the scaling with the separation between the components."
"We consider the connection between two constructions of the mirror partner for the calabi-yau orbifold. This orbifold is defined as a quotient by some suitable subgroup $g$ of the phase symmetries of the hypersurface $ x_m $ in the weighted projective space, cut out by a quasi-homogeneous polynomial $w_m$. The first, berglund-h\""ubsch-krawitz (bhk) construction, uses another weighted projective space and the quotient of a new hypersurface $x_{m^t}$ inside it by some dual group $g^t$. In the second, batyrev construction, the mirror partner is constructed as a hypersurface in the toric variety defined by the reflexive polytope dual to the polytope associated with the original calabi-yau orbifold. We give a simple evidence of the equivalence of these two constructions."
"In this work, a generalization of the study of the human gait was made from already existent models in the literature, like models of keller and kockshenev. In this hybrid model, a strategy of metabolic energy minimization is combined in a race process, with a non-linear description of the movement of the mass center's libration, trying to reproduce the behavior of the walk-run transition. The results of the experimental data, for different speed regimes, indicate that the perimeter of the trajectory of the mass center is a relevant quantity in the quantification of this dynamic. An experimental procedure was put into practice in collaboration with the research group in biomedical engineering, basic sciences and laboratories of the manuela beltr\'an university in bogot\'a, colombia."
"We present a new mixed variable symplectic (mvs) integrator for planetary systems, that fully resolve close encounters. The method is based on a time regularisation that allows keeping the stability properties of the symplectic integrators, while also reducing the effective step size whenever two planets encounter. We use a high order mvs scheme such that it is possible to integrate with large time steps far away from close encounters. We show that this algorithm is able to resolve almost exact collisions (i.e with a mutual separation of a fraction of the physical radius) while using the same time-step as in weakly perturbed problem such as the solar system. We demonstrate the long term behaviour on systems of six super-earths experiencing strong scattering for 50 kyr. We compare our algorithm to hybrid methods such as mercury and show that for an equivalent cost we obtain much better energy conservation."
"Entanglement is a key resource for quantum information processing. A widely used tool for detecting entanglement is entanglement witness, where the measurement of the witness operator is guaranteed to be positive for all separable states and can be negative for certain entangled states. In reality, due to the exponentially increasing the hilbert-space dimension with respective to the system size, it is very challenging to construct an efficient entanglement witness for general multipartite entangled states. For $n$-partite greenberger-horne-zeilinger (ghz)-like states, the most robust witness scheme requires $n+1$ local measurement settings and can tolerate up to $1/2$ white noise. As a comparison, the most efficient witness for ghz-like states only needs two local measurement settings and can tolerate up to $1/3$ white noise."
"Within general relativity, the unique stationary solution of an isolated black hole is the kerr spacetime, which has a peculiar multipolar structure depending only on its mass and spin. We develop a general method to extract the multipole moments of arbitrary stationary spacetimes and apply it to a large family of horizonless microstate geometries. The latter can break the axial and equatorial symmetry of the kerr metric and have a much richer multipolar structure, which provides a portal to constrain fuzzball models phenomenologically. We find numerical evidence that all multipole moments are typically larger (in absolute value) than those of a kerr black hole with the same mass and spin. Current measurements of the quadrupole moment of black-hole candidates could place only mild constraints on fuzzballs, while future gravitational-wave detections of extreme mass-ratio inspirals with the space mission lisa will improve these bounds by orders of magnitude."
"Reasoning about graphs evolving over time is a challenging concept in many domains, such as bioinformatics, physics, and social networks. We consider a common case in which edges can be short term interactions (e.g., messaging) or long term structural connections (e.g., friendship). In practice, long term edges are often specified by humans. Human-specified edges can be both expensive to produce and suboptimal for the downstream task. To alleviate these issues, we propose a model based on temporal point processes and variational autoencoders that learns to infer temporal attention between nodes by observing node communication. As temporal attention drives between-node feature propagation, using the dynamics of node interactions to learn this key component provides more flexibility while simultaneously avoiding issues associated with human-specified edges."
"Observations of thermally driven transverse vibration of a photonic crystal waveguide (pcw) are reported. The pcw consists of two parallel nanobeams with a 240 nm vacuum gap between the beams. Models are developed and validated for the transduction of beam motion to phase and amplitude modulation of a weak optical probe propagating in a guided mode (gm) of the pcw for probe frequencies far from and near to the dielectric band edge. Since our pcw has been designed for near-field atom trapping, this research provides a foundation for evaluating possible deleterious effects of thermal motion on optical atomic traps near the surfaces of pcws. Longer term goals are to achieve strong atom-mediated links between individual phonons of vibration and single photons propagating in the gms of the pcw, thereby enabling opto-mechanics at the quantum level with atoms, photons, and phonons."
"We establish natural splittings for the values of global mackey functors at orthogonal, unitary and symplectic groups. In particular, the restriction homomorphisms between the orthogonal, unitary and symplectic groups of adjacent dimensions are naturally split epimorphisms. The interest in the splitting comes from equivariant stable homotopy theory. The equivariant stable homotopy groups of every global spectrum form a global mackey functor, so the splittings imply that certain long exact homotopy group sequences separate into short exact sequences. For the real and complex global thom spectra $\mathbf{mo}$ and $\mathbf{mu}$, the splittings imply the regularity of various euler classes related to the tautological representations of $o(n)$ and $u(n)$."
"Baryonic feedback effects lead to a suppression of the weak lensing angular power spectrum on small scales. The poorly constrained shape and amplitude of this suppression is an important source of uncertainties for upcoming cosmological weak lensing surveys such as euclid or lsst. In this first paper in a series of two, we use simulations to build a euclid-like tomographic mock data-set for the cosmic shear power spectrum and the corresponding covariance matrix, which are both corrected for baryonic effects following the baryonification method of schneider et al. (2019). In addition, we develop an emulator to obtain fast predictions of the baryonic power suppression, allowing us to perform a likelihood inference analysis for a standard $\lambda$cdm cosmology with both cosmological and astrophysical parameters."
"Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages."
"Density-based clustering relies on the idea of linking groups to some specific features of the probability distribution underlying the data. The reference to a true, yet unknown, population structure allows to frame the clustering problem in a standard inferential setting, where the concept of ideal population clustering is defined as the partition induced by the true density function. The nonparametric formulation of this approach, known as modal clustering, draws a correspondence between the groups and the domains of attraction of the density modes. Operationally, a nonparametric density estimate is required and a proper selection of the amount of smoothing, governing the shape of the density and hence possibly the modal structure, is crucial to identify the final partition. In this work, we address the issue of density estimation for modal clustering from an asymptotic perspective."
"The tails of diboson production at the lhc are sensitive to the interference between standard model and higher dimension operators parameterizing the effects of heavy new physics. However, helicity selection rules for the diboson scattering amplitudes set an obstruction to the na\""ive interference contributions of dimension six operators, causing the total diboson rate correction's leading contribution to cancel. In this case, carefully measuring the azimuthal decay angles ""resurrects"" the interference, recouping sensitivity to the ""non-interfering"" operators. We explore these signatures in detail, and find that the eft uncertainties associated with higher-dimensional operators are uniquely well-suppressed by the construction of an asymmetry variable which is only generated by these non-interfering operators, relegating the effects of higher-dimensional, interfering operators to the same status as statistical errors in this observable."
"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science."
"The weak gravity conjecture (wgc) demands the existence of superextremal particles in any consistent quantum theory of gravity. The standard lore is that these particles are introduced to ensure that extremal black holes are either unstable or marginally stable, but it is not clear what is wrong if this doesn't happen. This note shows that, for a generic einstein quantum theory of gravity in ads, exactly stability of extremal black branes is in tension with rigorously proven quantum information theorems about entanglement entropy. Avoiding the contradiction leads to a nonperturbative version of the wgc, which reduces to the usual statement at weak coupling. The argument is general, and it does not rely on either supersymmetry or a particular uv completion, assuming only the validity of einsteinian gravity, effective field theory, and holography."
"Understanding how strongly correlated two-dimensional (2d) systems can give rise to unconventional superconductivity with high critical temperatures is one of the major unsolved problems in condensed matter physics. Ultracold 2d fermi gases have emerged as clean and controllable model systems to study the interplay of strong correlations and reduced dimensionality, but direct evidence of superfluidity in these systems has been missing. Here, we demonstrate superfluidity in an ultracold 2d fermi gas by moving a periodic potential through the system and observing no dissipation below a critical velocity v$_{\rm c}$. We measure v$_{\rm c}$ as a function of interaction strength and find a maximum in the crossover regime between bosonic and fermionic superfluidity. Our measurement establishes ultracold fermi gases as a powerful tool for studying the influence of reduced dimensionality on strongly correlated superfluids."
"Ordinary differential equation (ode) is a mathematical model used in many application areas such as climatology, bioinformatics, and chemical engineering with its intuitive appeal to modeling. Despite ode's wide usage in modeling, frequent absence of their analytic solutions makes it difficult to estimate ode parameters from the data, especially when the model has lots of variables and parameters. This paper proposes a bayesian ode parameter estimating algorithm which is fast and accurate even for models with many parameters. The proposed method approximates an ode model with a state-space model based on equations of a numeric solver. It allows fast estimation by avoiding computations of a whole numerical solution in the likelihood. The posterior is obtained by a variational bayes method, more specifically, the approximate riemannian conjugate gradient method (honkela et al."
"We study the transport properties and superconducting proximity effect in nsn junctions formed by a time-reversal symmetry broken weyl semimetal (wsm) in proximity to an $s$-wave superconductor. We find that the differential conductance and induced pairing amplitudes strongly depend on the angle between the junction direction in real space and the axis separating the weyl nodes in momentum space. We identify the influence of a chiral chemical potential, i.e., the electron population imbalance between weyl nodes of opposite chirality, on the transport characteristics of the junction. Remarkably, we observe a net spin polarization of cooper pairs that are generated via andreev reflection in the two wsm regions. The spin polarization is opposite in the two wsm regions and highly sensitive to the chirality imbalance and excitation energy."
"Metal ion implantation into ceramics has been demonstrated to be an effective and controllable technique for tailoring the surface electrical conductivity of the ceramic piece, and this approach has been used in a number of applications. Importantly, it provides a method for grading the voltage drop across high voltage insulators, and thereby increasing the maximum operational voltage that can be applied across the insulator without surface flashover. However a concern for the use of the method is the long term stability of the implantation-induced conductivity, this especially so if the implanted metal species is readily oxidized. Here we report on our examination of the long-term behavior of the surface conductivity of titanium-implanted alumina. The results indicate that after an initial drop of as much as 40% in the first few weeks after implantation, the conductivity shows only a very slow decrease of about 10% over the following year."
"We introduce a novel skyrme-like conserved current in the effective theory of pions and vector mesons based on the idea of hidden local symmetry. The associated charge is equivalent to the skyrmion charge for any smooth configuration. In addition, there exist singular configurations that can be identified as n_f=1 baryons charged under the new symmetry. Under this identification, the vector mesons play the role of the chern-simons vector fields living on the quantum hall droplet that forms the n_f=1 baryon. We propose that this current is the correct effective expression for the baryon current at low energies. This proposal gives a unified picture for the two types of baryons and allows them to continuously transform one to the other in a natural way. In addition, chern-simons dualities on the droplet can be interpreted as a result of seiberg-like duality between gluons and vector mesons."
"Interfacial spin-orbit coupling in josephson junctions offers an intriguing way to combine anomalous hall and josephson physics in a single device. We study theoretically how the superposition of both effects impacts superconductor/ferromagnetic insulator/superconductor junctions' transport properties. Transverse momentum-dependent skew tunneling of cooper pairs through the spin-active ferromagnetic insulator interface creates sizable transverse hall supercurrents, to which we refer as anomalous josephson hall effect currents. We generalize the furusaki-tsukada formula, which got initially established to quantify usual (tunneling) josephson current flows, to evaluate the transverse current components and demonstrate that their amplitudes are widely adjustable by means of the spin-orbit coupling strengths or the superconducting phase difference across the junction. As a clear spectroscopic fingerprint of josephson junctions, well-localized subgap bound states form around the interface."
"Following the suggestion from the monte--carlo experiments in jim\'enez, j. Of turbul. 2020), that dipoles are as important to the dynamics of decaying two-dimensional turbulence as individual vortex cores, it is found that the kinetic energy of this flow is carried by elongated streams formed by the concatenation of dipoles. Vortices separate into a family of small fast-moving cores, and another family of larger slowly moving ones, which can be described as `frozen' into a slowly evolving `crystal'. The kinematics of both families are very different, and only the former is self-similar. The latter is responsible for most of the kinetic energy of the flow, and its vortices form the dipoles and the streams. Mechanisms are discussed for the growth of this slow component."
"Continuous-time quantum walks have proven to be an extremely useful framework for the design of several quantum algorithms. Often, the running time of quantum algorithms in this framework is characterized by the quantum hitting time: the time required by the quantum walk to find a vertex of interest with a high probability. In this article, we provide improved upper bounds for the quantum hitting time that can be applied to several ctqw-based quantum algorithms. In particular, we apply our techniques to the glued-trees problem, improving their hitting time upper bound by a polynomial factor: from $o(n^5)$ to $o(n^2\log n)$. Furthermore, our methods also help to exponentially improve the dependence on precision of the continuous-time quantum walk based algorithm to find a marked node on any ergodic, reversible markov chain by chakraborty et al."
"Extreme mass ratio inspirals (emris) can be classified as dry emris and wet emris based on their formation mechanisms. Dry (or the ""loss-cone"") emris, previsouly considered as the main emri sources for the laser interferometer space antenna, are primarily produced by multi-body scattering in the nuclear star cluster and gravitational capture. In this letter, we highlight an alternative emri formation channel: (wet) emri formation assisted by the accretion flow around accreting galactic-center massive black holes (mbhs). In this channel, the accretion disk captures stellar-mass black holes that are intially moving on inclined orbits, and subsequently drives them to migrate towards the mbh - this process boosts the formation rate of emris in such galaxies by orders of magnitude."
"When a new user just signs up on a website, we usually have no information about him/her, i.e. No interaction with items, no user profile and no social links with other users. Under such circumstances, we still expect our recommender systems could attract the users at the first time so that the users decide to stay on the website and become active users. This problem falls into new user cold-start category and it is crucial to the development and even survival of a company. Existing works on user cold-start recommendation either require additional user efforts, e.g. Setting up an interview process, or make use of side information [10] such as user demographics, locations, social relations, etc. However, users may not be willing to take the interview and side information on cold-start users is usually not available."
"Deep learning has solved many problems that are out of reach of heuristic algorithms. It has also been successfully applied in wireless communications, even though the current radio systems are well-understood and optimal algorithms exist for many tasks. While some gains have been obtained by learning individual parts of a receiver, a better approach is to jointly learn the whole receiver. This, however, often results in a challenging nonlinear problem, for which the optimal solution is infeasible to implement. To this end, we propose a deep fully convolutional neural network, deeprx, which executes the whole receiver pipeline from frequency domain signal stream to uncoded bits in a 5g-compliant fashion. We facilitate accurate channel estimation by constructing the input of the convolutional neural network in a very specific manner using both the data and pilot symbols."
"Mutation-based fuzzing typically uses an initial set of non-crashing seed inputs (a corpus) from which to generate new inputs by mutation. A corpus of potential seeds will often contain thousands of similar inputs. This lack of diversity can lead to wasted fuzzing effort by exhaustive mutation from all available seeds. To address this, fuzzers come with distillation tools (e.g., afl-cmin) that select the smallest subset of seeds that triggers the same range of instrumentation data points as the full corpus. Common practice suggests that minimizing the number and cumulative size of the seeds leads to more efficient fuzzing, which we explore systematically. We present results of 34+ cpu-years of fuzzing with five distillation approaches to understand their impact in finding bugs in real-world software."
"In this paper we investigate multi-agent discrete-event systems with partial observation. The agents can be divided into several groups in each of which the agents have similar (isomorphic) state transition structures, and thus can be relabeled into the same template. Based on the template a scalable supervisor whose state size and computational cost are independent of the number of agents is designed for the case of partial observation. The scalable supervisor under partial observation does not need to be recomputed regardless of how many agents are added to or removed from the system. We generalize our earlier results to partial observation by proposing sufficient conditions for safety and maximal permissiveness of the scalable least restrictive supervisor on the template level. An example is provided to illustrate the proposed scalable supervisory synthesis."
"Electric network frequency (enf) fluctuations constitute a powerful tool in multimedia forensics. An efficient approach for enf estimation is introduced with temporal windowing based on the filter-bank capon spectral estimator. A type of gohberg-semencul factorization of the model covariance matrix is used due to the toeplitz structure of the covariance matrix. Moreover, this approach uses, for the first time in the field of enf, a temporal window, not necessarily the rectangular one, at the stage preceding spectral estimation. Krylov matrices are employed for fast implementation of matrix inversions. The proposed approach outperforms the state-of-the-art methods in enf estimation, when a short time window of $1$ second is employed in power recordings. In speech recordings, the proposed approach yields highly accurate results with respect to both time complexity and accuracy."
"The direpack package aims to establish a set of modern statistical dimension reduction techniques into the python universe as a single, consistent package. The dimension reduction methods included resort into three categories: projection pursuit based dimension reduction, sufficient dimension reduction, and robust m estimators for dimension reduction. As a corollary, regularized regression estimators based on these reduced dimension spaces are provided as well, ranging from classical principal component regression up to sparse partial robust m regression. The package also contains a set of classical and robust pre-processing utilities, including generalized spatial signs, as well as dedicated plotting functionality and cross-validation utilities. Finally, direpack has been written consistent with the scikit-learn api, such that the estimators can flawlessly be included into (statistical and/or machine) learning pipelines in that framework."
"The nonlinear fluorescence emission has been widely applied for the high spatial resolution optical imaging. Here, we studied the fluorescence anomalous saturating effect of the nitrogen vacancy defect in diamond. The fluorescence reduction was observed with high power laser excitation. It increased the nonlinearity of the fluorescence emission, and changed the spatial frequency distribution of the fluorescence image. We used a differential excitation protocol to extract the high spatial frequency information. By modulating the excitation laser's power, the spatial resolution of imaging was improved approximate 1.6 times in comparison with the confocal microscopy. Due to the simplicity of the experimental setup and data processing, we expect this method can be used for improving the spatial resolution of sensing and biological labeling with the defects in solids."
"When random effects are correlated with sample design variables, the usual approach of employing individual survey weights (constructed to be inversely proportional to the unit survey inclusion probabilities) to form a pseudo-likelihood no longer produces asymptotically unbiased inference. We construct a weight-exponentiated formulation for the random effects distribution that achieves unbiased inference for generating hyperparameters of the random effects. We contrast our approach with frequentist methods that rely on numerical integration to reveal that only the bayesian method achieves both unbiased estimation with respect to the sampling design distribution and consistency with respect to the population generating distribution. Our simulations and real data example for a survey of business establishments demonstrate the utility of our approach across different modeling formulations and sampling designs. This work serves as a capstone for recent developmental efforts that combine traditional survey estimation approaches with the bayesian modeling paradigm and provides a bridge across the two rich but disparate sub-fields."
"Low-symmetry 2d materials---such as res$_2$ and rese$_2$ monolayers, black phosphorus monolayers, group-iv monochalcogenide monolayers, borophene, among others---have more complex atomistic structures than the honeycomb lattices of graphene, hexagonal boron nitride, and transition metal dichalcogenides. The reduced symmetries of these emerging materials give rise to inhomogeneous electron, optical, valley, and spin responses, as well as entirely new properties such as ferroelasticity, ferroelectricity, magnetism, spin-wave phenomena, large nonlinear optical properties, photogalvanic effects, and superconductivity. Novel electronic topological properties, nonlinear elastic properties, and structural phase transformations can also take place due to low symmetry. The ""beyond graphene: low-symmetry and anisotropic 2d materials"" special topic was assembled to highlight recent experimental and theoretical research on these emerging materials."
"Orientational dynamics in the isotropic phase of a comb-shaped nematic polymer with mesogenic and functional side groups was studied using the kerr effect and dielectric spectroscopy. For the first time, it was found that in a mesogenic polymer, in contrast to low-molecular-weight mesogens, the relaxation of the electric birefringence of a melt above the temperature of the nematic-isotropic phase transition can be presented by a sum of several exponential processes, two of which play a decisive role. These main processes replace each other in a temperature range of about fifty degrees. Dielectric spectroscopy also made it possible to distinguish two processes of orientational relaxation: the first is due to rotation of the side mesogenic groups, and the second is associated with motion of the main chain segments."
"Deep hashing methods have shown great retrieval accuracy and efficiency in large-scale image retrieval. How to optimize discrete hash bits is always the focus in deep hashing methods. A common strategy in these methods is to adopt an activation function, e.g. $\operatorname{sigmoid}(\cdot)$ or $\operatorname{tanh}(\cdot)$, and minimize a quantization loss to approximate discrete values. However, this paradigm may make more and more hash bits stuck into the wrong saturated area of the activation functions and never escaped. We call this problem ""dead bits problem~(dbp)"". Besides, the existing quantization loss will aggravate dbp as well. In this paper, we propose a simple but effective gradient amplifier which acts before activation functions to alleviate dbp."
"We study the phenomenology of a hypercharge-zero su(2) triplet scalar whose existence is motivated by two-step electroweak symmetry-breaking. We consider both the possibility that the triplets are stable and contribute to the dark matter density, or that they decay via mixing with the standard model higgs boson. The former is constrained by disappearing charged track searches at the lhc and by dark matter direct detection experiments, while the latter is constrained by existing multilepton collider searches. We find that a two-step electroweak phase transition involving a stable triplet with a negative quadratic term is ruled out by direct detection searches, while an unstable triplet with a mass less than $230\ \mathrm{gev}$ is excluded at $95\%$ confidence level."
"This paper shows that the eccentric debris rings seen around the stars fomalhaut and hd 202628 are narrower than expected in the standard eccentric planet perturbation scenario (sometimes referred to as ""pericenter glow""). The standard scenario posits an initially circular and narrow belt of planetesimals at semi-major axis $a$, whose eccentricity is increased to $e_f$ after the gas disc has dispersed by secular perturbations from an eccentric planet, resulting in a belt of width $2ae_f$. In a minor modification of this scenario, narrower belts can arise if the planetesimals are initially eccentric, which could result from earlier planet perturbations during the gas-rich protoplanetary disc phase. However, a primordial eccentricity could alternatively be caused by instabilities that increase the disc eccentricity, without the need for any planets."
"An electron is usually considered to have only one type of kinetic energy, but could it have more, for its spin and charge, or by exciting other electrons? in one dimension (1d), the physics of interacting electrons is captured well at low energies by the tomonaga-luttinger-liquid (tll) model, yet little has been observed experimentally beyond this linear regime. Here, we report on measurements of many-body modes in 1d gated-wires using a tunnelling spectroscopy technique. We observe two separate fermi seas at high energies, associated with spin and charge excitations, together with the emergence of three additional 1d 'replica' modes that strengthen with decreasing wire length. The effective interaction strength in the wires is varied by changing the amount of 1d inter-subband screening by over 45%."
"$context$. The assembly history experienced by the milky way is currently being unveiled thanks to the data provided by the $gaia$ mission. It is likely that the globular cluster system of our galaxy has followed a similarly intricate formation path. $aims$. To constrain this formation path, we explore the link between the globular clusters and the known merging events that the milky way has experienced. $methods$. To this end, we combined the kinematic information provided by $gaia$ for almost all galactic clusters, with the largest sample of cluster ages available after carefully correcting for systematic errors. To identify clusters with a common origin we analysed their dynamical properties, particularly in the space of integrals of motion. $results$. We find that about 40% of the clusters likely formed in situ."
"The estimation of the intrinsic dimension of a dataset is a fundamental step in most dimensionality reduction techniques. This article illustrates intrinsic, an r package that implements novel state-of-the-art likelihood-based estimators of the intrinsic dimension of a dataset. In detail, the methods included in this package are the two-nn, gride, and hidalgo models. To allow these novel estimators to be easily accessible, the package contains a few high-level, intuitive functions that rely on a broader set of efficient, low-level routines. Intrinsic encompasses models that fall into two categories: homogeneous and heterogeneous intrinsic dimension estimators. The first category contains the two-nn and gride models. The functions dedicated to these two methods carry out inference under both the frequentist and bayesian frameworks. In the second category we find hidalgo, a bayesian mixture model, for which an efficient gibbs sampler is implemented."
"Spintronic nanodevices have ultrafast nonlinear dynamic and recurrence behaviors on a nanosecond scale that promises to enable spintronic reservoir computing (rc) system. Here two physical rc systems based on a single magnetic skyrmion memristor (msm) and 24 spin-torque nano-oscillators (stnos) were proposed and modeled to process image classification task and nonlinear dynamic system prediction, respectively. Based on our micromagnetic simulation results on the nonlinear responses of msm and stno with current pulses stimulation, the handwritten digits recognition task domesticates that an rc system using one single msm has the outstanding performance on image classification. In addition, the complex unknown nonlinear dynamic problems can also be well solved by a physical rc system consisted of 24 stnos confirmed in a second-order nonlinear dynamic system and narma10 tasks."
"Ranking algorithms play a crucial role in online platforms ranging from search engines to recommender systems. In this paper, we identify a surprising consequence of popularity-based rankings: the fewer the items reporting a given signal, the higher the share of the overall traffic they collectively attract. This few-get-richer effect emerges in settings where there are few distinct classes of items (e.g., left-leaning news sources versus right-leaning news sources), and items are ranked based on their popularity. We demonstrate analytically that the few-get-richer effect emerges when people tend to click on top-ranked items and have heterogeneous preferences for the classes of items. Using simulations, we analyze how the strength of the effect changes with assumptions about the setting and human behavior. We also test our predictions experimentally in an online experiment with human participants. Our findings have important implications to understand the spread of misinformation."
"Weight pruning has been widely acknowledged as a straightforward and effective method to eliminate redundancy in deep neural networks (dnn), thereby achieving acceleration on various platforms. However, most of the pruning techniques are essentially trade-offs between model accuracy and regularity which lead to impaired inference accuracy and limited on-device acceleration performance. To solve the problem, we introduce a new sparsity dimension, namely pattern-based sparsity that comprises pattern and connectivity sparsity, and becoming both highly accurate and hardware friendly. With carefully designed patterns, the proposed pruning unprecedentedly and consistently achieves accuracy enhancement and better feature extraction ability on different dnn structures and datasets, and our pattern-aware pruning framework also achieves pattern library extraction, pattern selection, pattern and connectivity pruning and weight training simultaneously. Our approach on the new pattern-based sparsity naturally fits into compiler optimization for highly efficient dnn execution on mobile platforms."
"Difference-in-differences analysis with a control group that differs considerably from a treated group is vulnerable to bias from historical events that have different effects on the groups. Constructing a more closely matched control group by matching a subset of the overall control group to the treated group may result in less bias. We study this phenomenon in simulation studies. We study the effect of mountaintop removal mining (mrm) on mortality using a difference-in-differences analysis that makes use of the increase in mrm following the 1990 clean air act amendments. For a difference-in-differences analysis of the effect of mrm on mortality, we constructed a more closely matched control group and found a 95\% confidence interval that contains substantial adverse effects along with no effect and small beneficial effects."
"When constructing models to summarize clinical data to be used for simulations, it is good practice to evaluate the models for their capacity to reproduce the data. This can be done by means of visual predictive checks (vpc), which consist of (1) several reproductions of the original study by simulation from the model under evaluation, (2) calculating estimates of interest for each simulated study and (3) comparing the distribution of those estimates with the estimate from the original study. This procedure is a generic method that is straightforward to apply, in general. Here we consider the application of the method to time to event data and consider the special case when a time-varying covariate is not known or cannot be approximated after event time."
"Novel concepts, perspectives and challenges in measuring and controlling an open quantum system via sequential schemes are shown. We discuss how similar protocols, relying both on repeated quantum measurements and dynamical decoupling control pulses, can allow to: (i) confine and protect quantum dynamics from decoherence in accordance with the zeno physics. (ii) analytically predict the probability that a quantum system is transferred into a target quantum state by means of stochastic sequential measurements. (iii) optimally reconstruct the spectral density of environmental noise sources by orthogonalizing in the frequency domain the filter functions driving the designed quantum-sensor. The achievement of these tasks will enhance our capability to observe and manipulate open quantum systems, thus bringing advances to quantum science and technologies."
"This paper presents a millimeter-scale cmos 64$\times$64 single charged particle radiation detector system for external beam cancer radiotherapy. A 1$\times$1 $\mu m^2$ diode measures energy deposition by a single charged particle in the depletion region, and the array design provides a large detection area of 512$\times$512 $\mu m^2$. Instead of sensing the voltage drop caused by radiation, the proposed system measures the pulse width, i.e., the time it takes for the voltage to return to its baseline. This obviates the need for using power-hungry and large analog-to-digital converters. A prototype asic is fabricated in tsmc 65 nm lp cmos process and consumes the average static power of 0.535 mw under 1.2 v analog and digital power supply. The functionality of the whole system is successfully verified in a clinical 67.5 mev proton beam setting."
"Routing controllability of autonomous vehicles (avs) has been shown to reduce the impact of selfish routing on network efficiency. However, the assumption that avs would readily allow themselves to be controlled externally by a central agency is unrealistic. In this paper, we propose a joint routing and pricing control scheme that aims to incentivize avs to seek centrally controlled system optimal (so) routing by saving on tolls while user equilibrium (ue) seeking avs and human-driven vehicles (hvs) are subject to a congestion charge. The problem is formulated as a bi-level optimization, in which dynamic tolls are optimized in the upper level, whereas the lower level is a mixed equilibrium simulation-based dynamic traffic assignment model considering mixed fleet of avs and hvs."
"Optomechanics and electromechanics have made it possible to prepare macroscopic mechanical oscillators in their quantum ground states, in quadrature squeezed states, and in entangled states of motion. In addition to coaxing ever larger and more tangible objects into a regime of quantum behavior, this new capability has encouraged ideas of using mechanical oscillators in the processing and communication of quantum information and as precision force sensors operating beyond the standard quantum limit. But the effectively linear interaction between motion and light or electricity precludes access to the broader class of quantum states of motion, such as cat states or energy squeezed states. Indeed, early optomechanical proposals noted the possibility to escape this restriction by creating strong quadratic coupling of motion to light. Although there have been experimental demonstrations of quadratically coupled optomechanical systems, these have not yet accessed nonclassical states of motion."
"Online reviews provide product evaluations for customers to make decisions. Unfortunately, the evaluations can be manipulated using fake reviews (""spams"") by professional spammers, who have learned increasingly insidious and powerful spamming strategies by adapting to the deployed detectors. Spamming strategies are hard to capture, as they can be varying quickly along time, different across spammers and target products, and more critically, remained unknown in most cases. Furthermore, most existing detectors focus on detection accuracy, which is not well-aligned with the goal of maintaining the trustworthiness of product evaluations. To address the challenges, we formulate a minimax game where the spammers and spam detectors compete with each other on their practical goals that are not solely based on detection accuracy. Nash equilibria of the game lead to stable detectors that are agnostic to any mixed detection strategies."
"We developed single-shot shaped pulses for ultra high fidelity (uh-fidelity) population transfer on a 3-level quantum system in lambda configuration. To ensure high fidelity, we use the lewis-riesenfeld (l-r) method to derive a family of solutions leading to an exact transfer, where the solutions follow a single dynamical mode of the l-r invariant. Among this family, we identify a tracking solution with a single parameter to control simultaneously the fidelity of the transfer, the population of the excited state, and robustness. We define a measure of the robustness of an uh-fidelity transfer as the minimum percentile deviation on the pulse areas at which the infidelity rises above $10^{-4}$. The robustness of our shaped pulses is found superior to that of gaussian and adiabatically-optimized pulses for moderate pulse areas."
"In this paper, we study the landscape of the population negative log-likelihood function of gaussian mixture models with a general number of components. Due to nonconvexity, there exist multiple local minima that are not globally optimal, even when the mixture is well-separated. We show that all local minima share the same form of structure that partially identifies the component centers of the true mixture, in the sense that each local minimum involves a non-overlapping combination of fitting multiple gaussians to a single true component and fitting a single gaussian to multiple true components. Our results apply to the setting where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is over-or under-specified. For gaussian mixtures with three components, we obtain sharper results in terms of the scaling with the separation between the components."
"We consider the connection between two constructions of the mirror partner for the calabi-yau orbifold. This orbifold is defined as a quotient by some suitable subgroup $g$ of the phase symmetries of the hypersurface $ x_m $ in the weighted projective space, cut out by a quasi-homogeneous polynomial $w_m$. The first, berglund-h\""ubsch-krawitz (bhk) construction, uses another weighted projective space and the quotient of a new hypersurface $x_{m^t}$ inside it by some dual group $g^t$. In the second, batyrev construction, the mirror partner is constructed as a hypersurface in the toric variety defined by the reflexive polytope dual to the polytope associated with the original calabi-yau orbifold. We give a simple evidence of the equivalence of these two constructions."
"In this work, a generalization of the study of the human gait was made from already existent models in the literature, like models of keller and kockshenev. In this hybrid model, a strategy of metabolic energy minimization is combined in a race process, with a non-linear description of the movement of the mass center's libration, trying to reproduce the behavior of the walk-run transition. The results of the experimental data, for different speed regimes, indicate that the perimeter of the trajectory of the mass center is a relevant quantity in the quantification of this dynamic. An experimental procedure was put into practice in collaboration with the research group in biomedical engineering, basic sciences and laboratories of the manuela beltr\'an university in bogot\'a, colombia."
"We present a new mixed variable symplectic (mvs) integrator for planetary systems, that fully resolve close encounters. The method is based on a time regularisation that allows keeping the stability properties of the symplectic integrators, while also reducing the effective step size whenever two planets encounter. We use a high order mvs scheme such that it is possible to integrate with large time steps far away from close encounters. We show that this algorithm is able to resolve almost exact collisions (i.e with a mutual separation of a fraction of the physical radius) while using the same time-step as in weakly perturbed problem such as the solar system. We demonstrate the long term behaviour on systems of six super-earths experiencing strong scattering for 50 kyr. We compare our algorithm to hybrid methods such as mercury and show that for an equivalent cost we obtain much better energy conservation."
"Entanglement is a key resource for quantum information processing. A widely used tool for detecting entanglement is entanglement witness, where the measurement of the witness operator is guaranteed to be positive for all separable states and can be negative for certain entangled states. In reality, due to the exponentially increasing the hilbert-space dimension with respective to the system size, it is very challenging to construct an efficient entanglement witness for general multipartite entangled states. For $n$-partite greenberger-horne-zeilinger (ghz)-like states, the most robust witness scheme requires $n+1$ local measurement settings and can tolerate up to $1/2$ white noise. As a comparison, the most efficient witness for ghz-like states only needs two local measurement settings and can tolerate up to $1/3$ white noise."
"Within general relativity, the unique stationary solution of an isolated black hole is the kerr spacetime, which has a peculiar multipolar structure depending only on its mass and spin. We develop a general method to extract the multipole moments of arbitrary stationary spacetimes and apply it to a large family of horizonless microstate geometries. The latter can break the axial and equatorial symmetry of the kerr metric and have a much richer multipolar structure, which provides a portal to constrain fuzzball models phenomenologically. We find numerical evidence that all multipole moments are typically larger (in absolute value) than those of a kerr black hole with the same mass and spin. Current measurements of the quadrupole moment of black-hole candidates could place only mild constraints on fuzzballs, while future gravitational-wave detections of extreme mass-ratio inspirals with the space mission lisa will improve these bounds by orders of magnitude."
"Reasoning about graphs evolving over time is a challenging concept in many domains, such as bioinformatics, physics, and social networks. We consider a common case in which edges can be short term interactions (e.g., messaging) or long term structural connections (e.g., friendship). In practice, long term edges are often specified by humans. Human-specified edges can be both expensive to produce and suboptimal for the downstream task. To alleviate these issues, we propose a model based on temporal point processes and variational autoencoders that learns to infer temporal attention between nodes by observing node communication. As temporal attention drives between-node feature propagation, using the dynamics of node interactions to learn this key component provides more flexibility while simultaneously avoiding issues associated with human-specified edges."
"Observations of thermally driven transverse vibration of a photonic crystal waveguide (pcw) are reported. The pcw consists of two parallel nanobeams with a 240 nm vacuum gap between the beams. Models are developed and validated for the transduction of beam motion to phase and amplitude modulation of a weak optical probe propagating in a guided mode (gm) of the pcw for probe frequencies far from and near to the dielectric band edge. Since our pcw has been designed for near-field atom trapping, this research provides a foundation for evaluating possible deleterious effects of thermal motion on optical atomic traps near the surfaces of pcws. Longer term goals are to achieve strong atom-mediated links between individual phonons of vibration and single photons propagating in the gms of the pcw, thereby enabling opto-mechanics at the quantum level with atoms, photons, and phonons."
"We establish natural splittings for the values of global mackey functors at orthogonal, unitary and symplectic groups. In particular, the restriction homomorphisms between the orthogonal, unitary and symplectic groups of adjacent dimensions are naturally split epimorphisms. The interest in the splitting comes from equivariant stable homotopy theory. The equivariant stable homotopy groups of every global spectrum form a global mackey functor, so the splittings imply that certain long exact homotopy group sequences separate into short exact sequences. For the real and complex global thom spectra $\mathbf{mo}$ and $\mathbf{mu}$, the splittings imply the regularity of various euler classes related to the tautological representations of $o(n)$ and $u(n)$."
"Baryonic feedback effects lead to a suppression of the weak lensing angular power spectrum on small scales. The poorly constrained shape and amplitude of this suppression is an important source of uncertainties for upcoming cosmological weak lensing surveys such as euclid or lsst. In this first paper in a series of two, we use simulations to build a euclid-like tomographic mock data-set for the cosmic shear power spectrum and the corresponding covariance matrix, which are both corrected for baryonic effects following the baryonification method of schneider et al. (2019). In addition, we develop an emulator to obtain fast predictions of the baryonic power suppression, allowing us to perform a likelihood inference analysis for a standard $\lambda$cdm cosmology with both cosmological and astrophysical parameters."
"Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages."
"Density-based clustering relies on the idea of linking groups to some specific features of the probability distribution underlying the data. The reference to a true, yet unknown, population structure allows to frame the clustering problem in a standard inferential setting, where the concept of ideal population clustering is defined as the partition induced by the true density function. The nonparametric formulation of this approach, known as modal clustering, draws a correspondence between the groups and the domains of attraction of the density modes. Operationally, a nonparametric density estimate is required and a proper selection of the amount of smoothing, governing the shape of the density and hence possibly the modal structure, is crucial to identify the final partition. In this work, we address the issue of density estimation for modal clustering from an asymptotic perspective."
"The tails of diboson production at the lhc are sensitive to the interference between standard model and higher dimension operators parameterizing the effects of heavy new physics. However, helicity selection rules for the diboson scattering amplitudes set an obstruction to the na\""ive interference contributions of dimension six operators, causing the total diboson rate correction's leading contribution to cancel. In this case, carefully measuring the azimuthal decay angles ""resurrects"" the interference, recouping sensitivity to the ""non-interfering"" operators. We explore these signatures in detail, and find that the eft uncertainties associated with higher-dimensional operators are uniquely well-suppressed by the construction of an asymmetry variable which is only generated by these non-interfering operators, relegating the effects of higher-dimensional, interfering operators to the same status as statistical errors in this observable."
"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science."
"The weak gravity conjecture (wgc) demands the existence of superextremal particles in any consistent quantum theory of gravity. The standard lore is that these particles are introduced to ensure that extremal black holes are either unstable or marginally stable, but it is not clear what is wrong if this doesn't happen. This note shows that, for a generic einstein quantum theory of gravity in ads, exactly stability of extremal black branes is in tension with rigorously proven quantum information theorems about entanglement entropy. Avoiding the contradiction leads to a nonperturbative version of the wgc, which reduces to the usual statement at weak coupling. The argument is general, and it does not rely on either supersymmetry or a particular uv completion, assuming only the validity of einsteinian gravity, effective field theory, and holography."
"Understanding how strongly correlated two-dimensional (2d) systems can give rise to unconventional superconductivity with high critical temperatures is one of the major unsolved problems in condensed matter physics. Ultracold 2d fermi gases have emerged as clean and controllable model systems to study the interplay of strong correlations and reduced dimensionality, but direct evidence of superfluidity in these systems has been missing. Here, we demonstrate superfluidity in an ultracold 2d fermi gas by moving a periodic potential through the system and observing no dissipation below a critical velocity v$_{\rm c}$. We measure v$_{\rm c}$ as a function of interaction strength and find a maximum in the crossover regime between bosonic and fermionic superfluidity. Our measurement establishes ultracold fermi gases as a powerful tool for studying the influence of reduced dimensionality on strongly correlated superfluids."
"Ordinary differential equation (ode) is a mathematical model used in many application areas such as climatology, bioinformatics, and chemical engineering with its intuitive appeal to modeling. Despite ode's wide usage in modeling, frequent absence of their analytic solutions makes it difficult to estimate ode parameters from the data, especially when the model has lots of variables and parameters. This paper proposes a bayesian ode parameter estimating algorithm which is fast and accurate even for models with many parameters. The proposed method approximates an ode model with a state-space model based on equations of a numeric solver. It allows fast estimation by avoiding computations of a whole numerical solution in the likelihood. The posterior is obtained by a variational bayes method, more specifically, the approximate riemannian conjugate gradient method (honkela et al."
"We study the transport properties and superconducting proximity effect in nsn junctions formed by a time-reversal symmetry broken weyl semimetal (wsm) in proximity to an $s$-wave superconductor. We find that the differential conductance and induced pairing amplitudes strongly depend on the angle between the junction direction in real space and the axis separating the weyl nodes in momentum space. We identify the influence of a chiral chemical potential, i.e., the electron population imbalance between weyl nodes of opposite chirality, on the transport characteristics of the junction. Remarkably, we observe a net spin polarization of cooper pairs that are generated via andreev reflection in the two wsm regions. The spin polarization is opposite in the two wsm regions and highly sensitive to the chirality imbalance and excitation energy."
"Metal ion implantation into ceramics has been demonstrated to be an effective and controllable technique for tailoring the surface electrical conductivity of the ceramic piece, and this approach has been used in a number of applications. Importantly, it provides a method for grading the voltage drop across high voltage insulators, and thereby increasing the maximum operational voltage that can be applied across the insulator without surface flashover. However a concern for the use of the method is the long term stability of the implantation-induced conductivity, this especially so if the implanted metal species is readily oxidized. Here we report on our examination of the long-term behavior of the surface conductivity of titanium-implanted alumina. The results indicate that after an initial drop of as much as 40% in the first few weeks after implantation, the conductivity shows only a very slow decrease of about 10% over the following year."
"We introduce a novel skyrme-like conserved current in the effective theory of pions and vector mesons based on the idea of hidden local symmetry. The associated charge is equivalent to the skyrmion charge for any smooth configuration. In addition, there exist singular configurations that can be identified as n_f=1 baryons charged under the new symmetry. Under this identification, the vector mesons play the role of the chern-simons vector fields living on the quantum hall droplet that forms the n_f=1 baryon. We propose that this current is the correct effective expression for the baryon current at low energies. This proposal gives a unified picture for the two types of baryons and allows them to continuously transform one to the other in a natural way. In addition, chern-simons dualities on the droplet can be interpreted as a result of seiberg-like duality between gluons and vector mesons."
"Interfacial spin-orbit coupling in josephson junctions offers an intriguing way to combine anomalous hall and josephson physics in a single device. We study theoretically how the superposition of both effects impacts superconductor/ferromagnetic insulator/superconductor junctions' transport properties. Transverse momentum-dependent skew tunneling of cooper pairs through the spin-active ferromagnetic insulator interface creates sizable transverse hall supercurrents, to which we refer as anomalous josephson hall effect currents. We generalize the furusaki-tsukada formula, which got initially established to quantify usual (tunneling) josephson current flows, to evaluate the transverse current components and demonstrate that their amplitudes are widely adjustable by means of the spin-orbit coupling strengths or the superconducting phase difference across the junction. As a clear spectroscopic fingerprint of josephson junctions, well-localized subgap bound states form around the interface."
"Following the suggestion from the monte--carlo experiments in jim\'enez, j. Of turbul. 2020), that dipoles are as important to the dynamics of decaying two-dimensional turbulence as individual vortex cores, it is found that the kinetic energy of this flow is carried by elongated streams formed by the concatenation of dipoles. Vortices separate into a family of small fast-moving cores, and another family of larger slowly moving ones, which can be described as `frozen' into a slowly evolving `crystal'. The kinematics of both families are very different, and only the former is self-similar. The latter is responsible for most of the kinetic energy of the flow, and its vortices form the dipoles and the streams. Mechanisms are discussed for the growth of this slow component."
"Continuous-time quantum walks have proven to be an extremely useful framework for the design of several quantum algorithms. Often, the running time of quantum algorithms in this framework is characterized by the quantum hitting time: the time required by the quantum walk to find a vertex of interest with a high probability. In this article, we provide improved upper bounds for the quantum hitting time that can be applied to several ctqw-based quantum algorithms. In particular, we apply our techniques to the glued-trees problem, improving their hitting time upper bound by a polynomial factor: from $o(n^5)$ to $o(n^2\log n)$. Furthermore, our methods also help to exponentially improve the dependence on precision of the continuous-time quantum walk based algorithm to find a marked node on any ergodic, reversible markov chain by chakraborty et al."
"Extreme mass ratio inspirals (emris) can be classified as dry emris and wet emris based on their formation mechanisms. Dry (or the ""loss-cone"") emris, previsouly considered as the main emri sources for the laser interferometer space antenna, are primarily produced by multi-body scattering in the nuclear star cluster and gravitational capture. In this letter, we highlight an alternative emri formation channel: (wet) emri formation assisted by the accretion flow around accreting galactic-center massive black holes (mbhs). In this channel, the accretion disk captures stellar-mass black holes that are intially moving on inclined orbits, and subsequently drives them to migrate towards the mbh - this process boosts the formation rate of emris in such galaxies by orders of magnitude."
"When a new user just signs up on a website, we usually have no information about him/her, i.e. No interaction with items, no user profile and no social links with other users. Under such circumstances, we still expect our recommender systems could attract the users at the first time so that the users decide to stay on the website and become active users. This problem falls into new user cold-start category and it is crucial to the development and even survival of a company. Existing works on user cold-start recommendation either require additional user efforts, e.g. Setting up an interview process, or make use of side information [10] such as user demographics, locations, social relations, etc. However, users may not be willing to take the interview and side information on cold-start users is usually not available."
"Deep learning has solved many problems that are out of reach of heuristic algorithms. It has also been successfully applied in wireless communications, even though the current radio systems are well-understood and optimal algorithms exist for many tasks. While some gains have been obtained by learning individual parts of a receiver, a better approach is to jointly learn the whole receiver. This, however, often results in a challenging nonlinear problem, for which the optimal solution is infeasible to implement. To this end, we propose a deep fully convolutional neural network, deeprx, which executes the whole receiver pipeline from frequency domain signal stream to uncoded bits in a 5g-compliant fashion. We facilitate accurate channel estimation by constructing the input of the convolutional neural network in a very specific manner using both the data and pilot symbols."
"Mutation-based fuzzing typically uses an initial set of non-crashing seed inputs (a corpus) from which to generate new inputs by mutation. A corpus of potential seeds will often contain thousands of similar inputs. This lack of diversity can lead to wasted fuzzing effort by exhaustive mutation from all available seeds. To address this, fuzzers come with distillation tools (e.g., afl-cmin) that select the smallest subset of seeds that triggers the same range of instrumentation data points as the full corpus. Common practice suggests that minimizing the number and cumulative size of the seeds leads to more efficient fuzzing, which we explore systematically. We present results of 34+ cpu-years of fuzzing with five distillation approaches to understand their impact in finding bugs in real-world software."
"In this paper we investigate multi-agent discrete-event systems with partial observation. The agents can be divided into several groups in each of which the agents have similar (isomorphic) state transition structures, and thus can be relabeled into the same template. Based on the template a scalable supervisor whose state size and computational cost are independent of the number of agents is designed for the case of partial observation. The scalable supervisor under partial observation does not need to be recomputed regardless of how many agents are added to or removed from the system. We generalize our earlier results to partial observation by proposing sufficient conditions for safety and maximal permissiveness of the scalable least restrictive supervisor on the template level. An example is provided to illustrate the proposed scalable supervisory synthesis."
"Electric network frequency (enf) fluctuations constitute a powerful tool in multimedia forensics. An efficient approach for enf estimation is introduced with temporal windowing based on the filter-bank capon spectral estimator. A type of gohberg-semencul factorization of the model covariance matrix is used due to the toeplitz structure of the covariance matrix. Moreover, this approach uses, for the first time in the field of enf, a temporal window, not necessarily the rectangular one, at the stage preceding spectral estimation. Krylov matrices are employed for fast implementation of matrix inversions. The proposed approach outperforms the state-of-the-art methods in enf estimation, when a short time window of $1$ second is employed in power recordings. In speech recordings, the proposed approach yields highly accurate results with respect to both time complexity and accuracy."
"The direpack package aims to establish a set of modern statistical dimension reduction techniques into the python universe as a single, consistent package. The dimension reduction methods included resort into three categories: projection pursuit based dimension reduction, sufficient dimension reduction, and robust m estimators for dimension reduction. As a corollary, regularized regression estimators based on these reduced dimension spaces are provided as well, ranging from classical principal component regression up to sparse partial robust m regression. The package also contains a set of classical and robust pre-processing utilities, including generalized spatial signs, as well as dedicated plotting functionality and cross-validation utilities. Finally, direpack has been written consistent with the scikit-learn api, such that the estimators can flawlessly be included into (statistical and/or machine) learning pipelines in that framework."
"The nonlinear fluorescence emission has been widely applied for the high spatial resolution optical imaging. Here, we studied the fluorescence anomalous saturating effect of the nitrogen vacancy defect in diamond. The fluorescence reduction was observed with high power laser excitation. It increased the nonlinearity of the fluorescence emission, and changed the spatial frequency distribution of the fluorescence image. We used a differential excitation protocol to extract the high spatial frequency information. By modulating the excitation laser's power, the spatial resolution of imaging was improved approximate 1.6 times in comparison with the confocal microscopy. Due to the simplicity of the experimental setup and data processing, we expect this method can be used for improving the spatial resolution of sensing and biological labeling with the defects in solids."
"When random effects are correlated with sample design variables, the usual approach of employing individual survey weights (constructed to be inversely proportional to the unit survey inclusion probabilities) to form a pseudo-likelihood no longer produces asymptotically unbiased inference. We construct a weight-exponentiated formulation for the random effects distribution that achieves unbiased inference for generating hyperparameters of the random effects. We contrast our approach with frequentist methods that rely on numerical integration to reveal that only the bayesian method achieves both unbiased estimation with respect to the sampling design distribution and consistency with respect to the population generating distribution. Our simulations and real data example for a survey of business establishments demonstrate the utility of our approach across different modeling formulations and sampling designs. This work serves as a capstone for recent developmental efforts that combine traditional survey estimation approaches with the bayesian modeling paradigm and provides a bridge across the two rich but disparate sub-fields."
"Low-symmetry 2d materials---such as res$_2$ and rese$_2$ monolayers, black phosphorus monolayers, group-iv monochalcogenide monolayers, borophene, among others---have more complex atomistic structures than the honeycomb lattices of graphene, hexagonal boron nitride, and transition metal dichalcogenides. The reduced symmetries of these emerging materials give rise to inhomogeneous electron, optical, valley, and spin responses, as well as entirely new properties such as ferroelasticity, ferroelectricity, magnetism, spin-wave phenomena, large nonlinear optical properties, photogalvanic effects, and superconductivity. Novel electronic topological properties, nonlinear elastic properties, and structural phase transformations can also take place due to low symmetry. The ""beyond graphene: low-symmetry and anisotropic 2d materials"" special topic was assembled to highlight recent experimental and theoretical research on these emerging materials."
"Orientational dynamics in the isotropic phase of a comb-shaped nematic polymer with mesogenic and functional side groups was studied using the kerr effect and dielectric spectroscopy. For the first time, it was found that in a mesogenic polymer, in contrast to low-molecular-weight mesogens, the relaxation of the electric birefringence of a melt above the temperature of the nematic-isotropic phase transition can be presented by a sum of several exponential processes, two of which play a decisive role. These main processes replace each other in a temperature range of about fifty degrees. Dielectric spectroscopy also made it possible to distinguish two processes of orientational relaxation: the first is due to rotation of the side mesogenic groups, and the second is associated with motion of the main chain segments."
"Deep hashing methods have shown great retrieval accuracy and efficiency in large-scale image retrieval. How to optimize discrete hash bits is always the focus in deep hashing methods. A common strategy in these methods is to adopt an activation function, e.g. $\operatorname{sigmoid}(\cdot)$ or $\operatorname{tanh}(\cdot)$, and minimize a quantization loss to approximate discrete values. However, this paradigm may make more and more hash bits stuck into the wrong saturated area of the activation functions and never escaped. We call this problem ""dead bits problem~(dbp)"". Besides, the existing quantization loss will aggravate dbp as well. In this paper, we propose a simple but effective gradient amplifier which acts before activation functions to alleviate dbp."
"We study the phenomenology of a hypercharge-zero su(2) triplet scalar whose existence is motivated by two-step electroweak symmetry-breaking. We consider both the possibility that the triplets are stable and contribute to the dark matter density, or that they decay via mixing with the standard model higgs boson. The former is constrained by disappearing charged track searches at the lhc and by dark matter direct detection experiments, while the latter is constrained by existing multilepton collider searches. We find that a two-step electroweak phase transition involving a stable triplet with a negative quadratic term is ruled out by direct detection searches, while an unstable triplet with a mass less than $230\ \mathrm{gev}$ is excluded at $95\%$ confidence level."
"This paper shows that the eccentric debris rings seen around the stars fomalhaut and hd 202628 are narrower than expected in the standard eccentric planet perturbation scenario (sometimes referred to as ""pericenter glow""). The standard scenario posits an initially circular and narrow belt of planetesimals at semi-major axis $a$, whose eccentricity is increased to $e_f$ after the gas disc has dispersed by secular perturbations from an eccentric planet, resulting in a belt of width $2ae_f$. In a minor modification of this scenario, narrower belts can arise if the planetesimals are initially eccentric, which could result from earlier planet perturbations during the gas-rich protoplanetary disc phase. However, a primordial eccentricity could alternatively be caused by instabilities that increase the disc eccentricity, without the need for any planets."
"An electron is usually considered to have only one type of kinetic energy, but could it have more, for its spin and charge, or by exciting other electrons? in one dimension (1d), the physics of interacting electrons is captured well at low energies by the tomonaga-luttinger-liquid (tll) model, yet little has been observed experimentally beyond this linear regime. Here, we report on measurements of many-body modes in 1d gated-wires using a tunnelling spectroscopy technique. We observe two separate fermi seas at high energies, associated with spin and charge excitations, together with the emergence of three additional 1d 'replica' modes that strengthen with decreasing wire length. The effective interaction strength in the wires is varied by changing the amount of 1d inter-subband screening by over 45%."
"$context$. The assembly history experienced by the milky way is currently being unveiled thanks to the data provided by the $gaia$ mission. It is likely that the globular cluster system of our galaxy has followed a similarly intricate formation path. $aims$. To constrain this formation path, we explore the link between the globular clusters and the known merging events that the milky way has experienced. $methods$. To this end, we combined the kinematic information provided by $gaia$ for almost all galactic clusters, with the largest sample of cluster ages available after carefully correcting for systematic errors. To identify clusters with a common origin we analysed their dynamical properties, particularly in the space of integrals of motion. $results$. We find that about 40% of the clusters likely formed in situ."
"The estimation of the intrinsic dimension of a dataset is a fundamental step in most dimensionality reduction techniques. This article illustrates intrinsic, an r package that implements novel state-of-the-art likelihood-based estimators of the intrinsic dimension of a dataset. In detail, the methods included in this package are the two-nn, gride, and hidalgo models. To allow these novel estimators to be easily accessible, the package contains a few high-level, intuitive functions that rely on a broader set of efficient, low-level routines. Intrinsic encompasses models that fall into two categories: homogeneous and heterogeneous intrinsic dimension estimators. The first category contains the two-nn and gride models. The functions dedicated to these two methods carry out inference under both the frequentist and bayesian frameworks. In the second category we find hidalgo, a bayesian mixture model, for which an efficient gibbs sampler is implemented."
"Spintronic nanodevices have ultrafast nonlinear dynamic and recurrence behaviors on a nanosecond scale that promises to enable spintronic reservoir computing (rc) system. Here two physical rc systems based on a single magnetic skyrmion memristor (msm) and 24 spin-torque nano-oscillators (stnos) were proposed and modeled to process image classification task and nonlinear dynamic system prediction, respectively. Based on our micromagnetic simulation results on the nonlinear responses of msm and stno with current pulses stimulation, the handwritten digits recognition task domesticates that an rc system using one single msm has the outstanding performance on image classification. In addition, the complex unknown nonlinear dynamic problems can also be well solved by a physical rc system consisted of 24 stnos confirmed in a second-order nonlinear dynamic system and narma10 tasks."
"Ranking algorithms play a crucial role in online platforms ranging from search engines to recommender systems. In this paper, we identify a surprising consequence of popularity-based rankings: the fewer the items reporting a given signal, the higher the share of the overall traffic they collectively attract. This few-get-richer effect emerges in settings where there are few distinct classes of items (e.g., left-leaning news sources versus right-leaning news sources), and items are ranked based on their popularity. We demonstrate analytically that the few-get-richer effect emerges when people tend to click on top-ranked items and have heterogeneous preferences for the classes of items. Using simulations, we analyze how the strength of the effect changes with assumptions about the setting and human behavior. We also test our predictions experimentally in an online experiment with human participants. Our findings have important implications to understand the spread of misinformation."
"Weight pruning has been widely acknowledged as a straightforward and effective method to eliminate redundancy in deep neural networks (dnn), thereby achieving acceleration on various platforms. However, most of the pruning techniques are essentially trade-offs between model accuracy and regularity which lead to impaired inference accuracy and limited on-device acceleration performance. To solve the problem, we introduce a new sparsity dimension, namely pattern-based sparsity that comprises pattern and connectivity sparsity, and becoming both highly accurate and hardware friendly. With carefully designed patterns, the proposed pruning unprecedentedly and consistently achieves accuracy enhancement and better feature extraction ability on different dnn structures and datasets, and our pattern-aware pruning framework also achieves pattern library extraction, pattern selection, pattern and connectivity pruning and weight training simultaneously. Our approach on the new pattern-based sparsity naturally fits into compiler optimization for highly efficient dnn execution on mobile platforms."
"Difference-in-differences analysis with a control group that differs considerably from a treated group is vulnerable to bias from historical events that have different effects on the groups. Constructing a more closely matched control group by matching a subset of the overall control group to the treated group may result in less bias. We study this phenomenon in simulation studies. We study the effect of mountaintop removal mining (mrm) on mortality using a difference-in-differences analysis that makes use of the increase in mrm following the 1990 clean air act amendments. For a difference-in-differences analysis of the effect of mrm on mortality, we constructed a more closely matched control group and found a 95\% confidence interval that contains substantial adverse effects along with no effect and small beneficial effects."
"When constructing models to summarize clinical data to be used for simulations, it is good practice to evaluate the models for their capacity to reproduce the data. This can be done by means of visual predictive checks (vpc), which consist of (1) several reproductions of the original study by simulation from the model under evaluation, (2) calculating estimates of interest for each simulated study and (3) comparing the distribution of those estimates with the estimate from the original study. This procedure is a generic method that is straightforward to apply, in general. Here we consider the application of the method to time to event data and consider the special case when a time-varying covariate is not known or cannot be approximated after event time."
"Novel concepts, perspectives and challenges in measuring and controlling an open quantum system via sequential schemes are shown. We discuss how similar protocols, relying both on repeated quantum measurements and dynamical decoupling control pulses, can allow to: (i) confine and protect quantum dynamics from decoherence in accordance with the zeno physics. (ii) analytically predict the probability that a quantum system is transferred into a target quantum state by means of stochastic sequential measurements. (iii) optimally reconstruct the spectral density of environmental noise sources by orthogonalizing in the frequency domain the filter functions driving the designed quantum-sensor. The achievement of these tasks will enhance our capability to observe and manipulate open quantum systems, thus bringing advances to quantum science and technologies."
"This paper presents a millimeter-scale cmos 64$\times$64 single charged particle radiation detector system for external beam cancer radiotherapy. A 1$\times$1 $\mu m^2$ diode measures energy deposition by a single charged particle in the depletion region, and the array design provides a large detection area of 512$\times$512 $\mu m^2$. Instead of sensing the voltage drop caused by radiation, the proposed system measures the pulse width, i.e., the time it takes for the voltage to return to its baseline. This obviates the need for using power-hungry and large analog-to-digital converters. A prototype asic is fabricated in tsmc 65 nm lp cmos process and consumes the average static power of 0.535 mw under 1.2 v analog and digital power supply. The functionality of the whole system is successfully verified in a clinical 67.5 mev proton beam setting."
"Routing controllability of autonomous vehicles (avs) has been shown to reduce the impact of selfish routing on network efficiency. However, the assumption that avs would readily allow themselves to be controlled externally by a central agency is unrealistic. In this paper, we propose a joint routing and pricing control scheme that aims to incentivize avs to seek centrally controlled system optimal (so) routing by saving on tolls while user equilibrium (ue) seeking avs and human-driven vehicles (hvs) are subject to a congestion charge. The problem is formulated as a bi-level optimization, in which dynamic tolls are optimized in the upper level, whereas the lower level is a mixed equilibrium simulation-based dynamic traffic assignment model considering mixed fleet of avs and hvs."
"Optomechanics and electromechanics have made it possible to prepare macroscopic mechanical oscillators in their quantum ground states, in quadrature squeezed states, and in entangled states of motion. In addition to coaxing ever larger and more tangible objects into a regime of quantum behavior, this new capability has encouraged ideas of using mechanical oscillators in the processing and communication of quantum information and as precision force sensors operating beyond the standard quantum limit. But the effectively linear interaction between motion and light or electricity precludes access to the broader class of quantum states of motion, such as cat states or energy squeezed states. Indeed, early optomechanical proposals noted the possibility to escape this restriction by creating strong quadratic coupling of motion to light. Although there have been experimental demonstrations of quadratically coupled optomechanical systems, these have not yet accessed nonclassical states of motion."
"Online reviews provide product evaluations for customers to make decisions. Unfortunately, the evaluations can be manipulated using fake reviews (""spams"") by professional spammers, who have learned increasingly insidious and powerful spamming strategies by adapting to the deployed detectors. Spamming strategies are hard to capture, as they can be varying quickly along time, different across spammers and target products, and more critically, remained unknown in most cases. Furthermore, most existing detectors focus on detection accuracy, which is not well-aligned with the goal of maintaining the trustworthiness of product evaluations. To address the challenges, we formulate a minimax game where the spammers and spam detectors compete with each other on their practical goals that are not solely based on detection accuracy. Nash equilibria of the game lead to stable detectors that are agnostic to any mixed detection strategies."
"We developed single-shot shaped pulses for ultra high fidelity (uh-fidelity) population transfer on a 3-level quantum system in lambda configuration. To ensure high fidelity, we use the lewis-riesenfeld (l-r) method to derive a family of solutions leading to an exact transfer, where the solutions follow a single dynamical mode of the l-r invariant. Among this family, we identify a tracking solution with a single parameter to control simultaneously the fidelity of the transfer, the population of the excited state, and robustness. We define a measure of the robustness of an uh-fidelity transfer as the minimum percentile deviation on the pulse areas at which the infidelity rises above $10^{-4}$. The robustness of our shaped pulses is found superior to that of gaussian and adiabatically-optimized pulses for moderate pulse areas."
"In this paper, we study the landscape of the population negative log-likelihood function of gaussian mixture models with a general number of components. Due to nonconvexity, there exist multiple local minima that are not globally optimal, even when the mixture is well-separated. We show that all local minima share the same form of structure that partially identifies the component centers of the true mixture, in the sense that each local minimum involves a non-overlapping combination of fitting multiple gaussians to a single true component and fitting a single gaussian to multiple true components. Our results apply to the setting where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is over-or under-specified. For gaussian mixtures with three components, we obtain sharper results in terms of the scaling with the separation between the components."
"We consider the connection between two constructions of the mirror partner for the calabi-yau orbifold. This orbifold is defined as a quotient by some suitable subgroup $g$ of the phase symmetries of the hypersurface $ x_m $ in the weighted projective space, cut out by a quasi-homogeneous polynomial $w_m$. The first, berglund-h\""ubsch-krawitz (bhk) construction, uses another weighted projective space and the quotient of a new hypersurface $x_{m^t}$ inside it by some dual group $g^t$. In the second, batyrev construction, the mirror partner is constructed as a hypersurface in the toric variety defined by the reflexive polytope dual to the polytope associated with the original calabi-yau orbifold. We give a simple evidence of the equivalence of these two constructions."
"In this work, a generalization of the study of the human gait was made from already existent models in the literature, like models of keller and kockshenev. In this hybrid model, a strategy of metabolic energy minimization is combined in a race process, with a non-linear description of the movement of the mass center's libration, trying to reproduce the behavior of the walk-run transition. The results of the experimental data, for different speed regimes, indicate that the perimeter of the trajectory of the mass center is a relevant quantity in the quantification of this dynamic. An experimental procedure was put into practice in collaboration with the research group in biomedical engineering, basic sciences and laboratories of the manuela beltr\'an university in bogot\'a, colombia."
"We present a new mixed variable symplectic (mvs) integrator for planetary systems, that fully resolve close encounters. The method is based on a time regularisation that allows keeping the stability properties of the symplectic integrators, while also reducing the effective step size whenever two planets encounter. We use a high order mvs scheme such that it is possible to integrate with large time steps far away from close encounters. We show that this algorithm is able to resolve almost exact collisions (i.e with a mutual separation of a fraction of the physical radius) while using the same time-step as in weakly perturbed problem such as the solar system. We demonstrate the long term behaviour on systems of six super-earths experiencing strong scattering for 50 kyr. We compare our algorithm to hybrid methods such as mercury and show that for an equivalent cost we obtain much better energy conservation."
"Entanglement is a key resource for quantum information processing. A widely used tool for detecting entanglement is entanglement witness, where the measurement of the witness operator is guaranteed to be positive for all separable states and can be negative for certain entangled states. In reality, due to the exponentially increasing the hilbert-space dimension with respective to the system size, it is very challenging to construct an efficient entanglement witness for general multipartite entangled states. For $n$-partite greenberger-horne-zeilinger (ghz)-like states, the most robust witness scheme requires $n+1$ local measurement settings and can tolerate up to $1/2$ white noise. As a comparison, the most efficient witness for ghz-like states only needs two local measurement settings and can tolerate up to $1/3$ white noise."
"Within general relativity, the unique stationary solution of an isolated black hole is the kerr spacetime, which has a peculiar multipolar structure depending only on its mass and spin. We develop a general method to extract the multipole moments of arbitrary stationary spacetimes and apply it to a large family of horizonless microstate geometries. The latter can break the axial and equatorial symmetry of the kerr metric and have a much richer multipolar structure, which provides a portal to constrain fuzzball models phenomenologically. We find numerical evidence that all multipole moments are typically larger (in absolute value) than those of a kerr black hole with the same mass and spin. Current measurements of the quadrupole moment of black-hole candidates could place only mild constraints on fuzzballs, while future gravitational-wave detections of extreme mass-ratio inspirals with the space mission lisa will improve these bounds by orders of magnitude."
"Reasoning about graphs evolving over time is a challenging concept in many domains, such as bioinformatics, physics, and social networks. We consider a common case in which edges can be short term interactions (e.g., messaging) or long term structural connections (e.g., friendship). In practice, long term edges are often specified by humans. Human-specified edges can be both expensive to produce and suboptimal for the downstream task. To alleviate these issues, we propose a model based on temporal point processes and variational autoencoders that learns to infer temporal attention between nodes by observing node communication. As temporal attention drives between-node feature propagation, using the dynamics of node interactions to learn this key component provides more flexibility while simultaneously avoiding issues associated with human-specified edges."
"Observations of thermally driven transverse vibration of a photonic crystal waveguide (pcw) are reported. The pcw consists of two parallel nanobeams with a 240 nm vacuum gap between the beams. Models are developed and validated for the transduction of beam motion to phase and amplitude modulation of a weak optical probe propagating in a guided mode (gm) of the pcw for probe frequencies far from and near to the dielectric band edge. Since our pcw has been designed for near-field atom trapping, this research provides a foundation for evaluating possible deleterious effects of thermal motion on optical atomic traps near the surfaces of pcws. Longer term goals are to achieve strong atom-mediated links between individual phonons of vibration and single photons propagating in the gms of the pcw, thereby enabling opto-mechanics at the quantum level with atoms, photons, and phonons."
"We establish natural splittings for the values of global mackey functors at orthogonal, unitary and symplectic groups. In particular, the restriction homomorphisms between the orthogonal, unitary and symplectic groups of adjacent dimensions are naturally split epimorphisms. The interest in the splitting comes from equivariant stable homotopy theory. The equivariant stable homotopy groups of every global spectrum form a global mackey functor, so the splittings imply that certain long exact homotopy group sequences separate into short exact sequences. For the real and complex global thom spectra $\mathbf{mo}$ and $\mathbf{mu}$, the splittings imply the regularity of various euler classes related to the tautological representations of $o(n)$ and $u(n)$."
"Baryonic feedback effects lead to a suppression of the weak lensing angular power spectrum on small scales. The poorly constrained shape and amplitude of this suppression is an important source of uncertainties for upcoming cosmological weak lensing surveys such as euclid or lsst. In this first paper in a series of two, we use simulations to build a euclid-like tomographic mock data-set for the cosmic shear power spectrum and the corresponding covariance matrix, which are both corrected for baryonic effects following the baryonification method of schneider et al. (2019). In addition, we develop an emulator to obtain fast predictions of the baryonic power suppression, allowing us to perform a likelihood inference analysis for a standard $\lambda$cdm cosmology with both cosmological and astrophysical parameters."
"Spontaneous conversations in real-world settings such as those found in child-centered recordings have been shown to be amongst the most challenging audio files to process. Nevertheless, building speech processing models handling such a wide variety of conditions would be particularly useful for language acquisition studies in which researchers are interested in the quantity and quality of the speech that children hear and produce, as well as for early diagnosis and measuring effects of remediation. In this paper, we present our approach to designing an open-source neural network to classify audio segments into vocalizations produced by the child wearing the recording device, vocalizations produced by other children, adult male speech, and adult female speech. To this end, we gathered diverse child-centered corpora which sums up to a total of 260 hours of recordings and covers 10 languages."
"Density-based clustering relies on the idea of linking groups to some specific features of the probability distribution underlying the data. The reference to a true, yet unknown, population structure allows to frame the clustering problem in a standard inferential setting, where the concept of ideal population clustering is defined as the partition induced by the true density function. The nonparametric formulation of this approach, known as modal clustering, draws a correspondence between the groups and the domains of attraction of the density modes. Operationally, a nonparametric density estimate is required and a proper selection of the amount of smoothing, governing the shape of the density and hence possibly the modal structure, is crucial to identify the final partition. In this work, we address the issue of density estimation for modal clustering from an asymptotic perspective."
"The tails of diboson production at the lhc are sensitive to the interference between standard model and higher dimension operators parameterizing the effects of heavy new physics. However, helicity selection rules for the diboson scattering amplitudes set an obstruction to the na\""ive interference contributions of dimension six operators, causing the total diboson rate correction's leading contribution to cancel. In this case, carefully measuring the azimuthal decay angles ""resurrects"" the interference, recouping sensitivity to the ""non-interfering"" operators. We explore these signatures in detail, and find that the eft uncertainties associated with higher-dimensional operators are uniquely well-suppressed by the construction of an asymmetry variable which is only generated by these non-interfering operators, relegating the effects of higher-dimensional, interfering operators to the same status as statistical errors in this observable."
"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science."
"The weak gravity conjecture (wgc) demands the existence of superextremal particles in any consistent quantum theory of gravity. The standard lore is that these particles are introduced to ensure that extremal black holes are either unstable or marginally stable, but it is not clear what is wrong if this doesn't happen. This note shows that, for a generic einstein quantum theory of gravity in ads, exactly stability of extremal black branes is in tension with rigorously proven quantum information theorems about entanglement entropy. Avoiding the contradiction leads to a nonperturbative version of the wgc, which reduces to the usual statement at weak coupling. The argument is general, and it does not rely on either supersymmetry or a particular uv completion, assuming only the validity of einsteinian gravity, effective field theory, and holography."
"Understanding how strongly correlated two-dimensional (2d) systems can give rise to unconventional superconductivity with high critical temperatures is one of the major unsolved problems in condensed matter physics. Ultracold 2d fermi gases have emerged as clean and controllable model systems to study the interplay of strong correlations and reduced dimensionality, but direct evidence of superfluidity in these systems has been missing. Here, we demonstrate superfluidity in an ultracold 2d fermi gas by moving a periodic potential through the system and observing no dissipation below a critical velocity v$_{\rm c}$. We measure v$_{\rm c}$ as a function of interaction strength and find a maximum in the crossover regime between bosonic and fermionic superfluidity. Our measurement establishes ultracold fermi gases as a powerful tool for studying the influence of reduced dimensionality on strongly correlated superfluids."
"Ordinary differential equation (ode) is a mathematical model used in many application areas such as climatology, bioinformatics, and chemical engineering with its intuitive appeal to modeling. Despite ode's wide usage in modeling, frequent absence of their analytic solutions makes it difficult to estimate ode parameters from the data, especially when the model has lots of variables and parameters. This paper proposes a bayesian ode parameter estimating algorithm which is fast and accurate even for models with many parameters. The proposed method approximates an ode model with a state-space model based on equations of a numeric solver. It allows fast estimation by avoiding computations of a whole numerical solution in the likelihood. The posterior is obtained by a variational bayes method, more specifically, the approximate riemannian conjugate gradient method (honkela et al."
"We study the transport properties and superconducting proximity effect in nsn junctions formed by a time-reversal symmetry broken weyl semimetal (wsm) in proximity to an $s$-wave superconductor. We find that the differential conductance and induced pairing amplitudes strongly depend on the angle between the junction direction in real space and the axis separating the weyl nodes in momentum space. We identify the influence of a chiral chemical potential, i.e., the electron population imbalance between weyl nodes of opposite chirality, on the transport characteristics of the junction. Remarkably, we observe a net spin polarization of cooper pairs that are generated via andreev reflection in the two wsm regions. The spin polarization is opposite in the two wsm regions and highly sensitive to the chirality imbalance and excitation energy."
"Metal ion implantation into ceramics has been demonstrated to be an effective and controllable technique for tailoring the surface electrical conductivity of the ceramic piece, and this approach has been used in a number of applications. Importantly, it provides a method for grading the voltage drop across high voltage insulators, and thereby increasing the maximum operational voltage that can be applied across the insulator without surface flashover. However a concern for the use of the method is the long term stability of the implantation-induced conductivity, this especially so if the implanted metal species is readily oxidized. Here we report on our examination of the long-term behavior of the surface conductivity of titanium-implanted alumina. The results indicate that after an initial drop of as much as 40% in the first few weeks after implantation, the conductivity shows only a very slow decrease of about 10% over the following year."
"We introduce a novel skyrme-like conserved current in the effective theory of pions and vector mesons based on the idea of hidden local symmetry. The associated charge is equivalent to the skyrmion charge for any smooth configuration. In addition, there exist singular configurations that can be identified as n_f=1 baryons charged under the new symmetry. Under this identification, the vector mesons play the role of the chern-simons vector fields living on the quantum hall droplet that forms the n_f=1 baryon. We propose that this current is the correct effective expression for the baryon current at low energies. This proposal gives a unified picture for the two types of baryons and allows them to continuously transform one to the other in a natural way. In addition, chern-simons dualities on the droplet can be interpreted as a result of seiberg-like duality between gluons and vector mesons."
"Interfacial spin-orbit coupling in josephson junctions offers an intriguing way to combine anomalous hall and josephson physics in a single device. We study theoretically how the superposition of both effects impacts superconductor/ferromagnetic insulator/superconductor junctions' transport properties. Transverse momentum-dependent skew tunneling of cooper pairs through the spin-active ferromagnetic insulator interface creates sizable transverse hall supercurrents, to which we refer as anomalous josephson hall effect currents. We generalize the furusaki-tsukada formula, which got initially established to quantify usual (tunneling) josephson current flows, to evaluate the transverse current components and demonstrate that their amplitudes are widely adjustable by means of the spin-orbit coupling strengths or the superconducting phase difference across the junction. As a clear spectroscopic fingerprint of josephson junctions, well-localized subgap bound states form around the interface."
"Following the suggestion from the monte--carlo experiments in jim\'enez, j. Of turbul. 2020), that dipoles are as important to the dynamics of decaying two-dimensional turbulence as individual vortex cores, it is found that the kinetic energy of this flow is carried by elongated streams formed by the concatenation of dipoles. Vortices separate into a family of small fast-moving cores, and another family of larger slowly moving ones, which can be described as `frozen' into a slowly evolving `crystal'. The kinematics of both families are very different, and only the former is self-similar. The latter is responsible for most of the kinetic energy of the flow, and its vortices form the dipoles and the streams. Mechanisms are discussed for the growth of this slow component."
"Continuous-time quantum walks have proven to be an extremely useful framework for the design of several quantum algorithms. Often, the running time of quantum algorithms in this framework is characterized by the quantum hitting time: the time required by the quantum walk to find a vertex of interest with a high probability. In this article, we provide improved upper bounds for the quantum hitting time that can be applied to several ctqw-based quantum algorithms. In particular, we apply our techniques to the glued-trees problem, improving their hitting time upper bound by a polynomial factor: from $o(n^5)$ to $o(n^2\log n)$. Furthermore, our methods also help to exponentially improve the dependence on precision of the continuous-time quantum walk based algorithm to find a marked node on any ergodic, reversible markov chain by chakraborty et al."
"Extreme mass ratio inspirals (emris) can be classified as dry emris and wet emris based on their formation mechanisms. Dry (or the ""loss-cone"") emris, previsouly considered as the main emri sources for the laser interferometer space antenna, are primarily produced by multi-body scattering in the nuclear star cluster and gravitational capture. In this letter, we highlight an alternative emri formation channel: (wet) emri formation assisted by the accretion flow around accreting galactic-center massive black holes (mbhs). In this channel, the accretion disk captures stellar-mass black holes that are intially moving on inclined orbits, and subsequently drives them to migrate towards the mbh - this process boosts the formation rate of emris in such galaxies by orders of magnitude."
"When a new user just signs up on a website, we usually have no information about him/her, i.e. No interaction with items, no user profile and no social links with other users. Under such circumstances, we still expect our recommender systems could attract the users at the first time so that the users decide to stay on the website and become active users. This problem falls into new user cold-start category and it is crucial to the development and even survival of a company. Existing works on user cold-start recommendation either require additional user efforts, e.g. Setting up an interview process, or make use of side information [10] such as user demographics, locations, social relations, etc. However, users may not be willing to take the interview and side information on cold-start users is usually not available."
"Deep learning has solved many problems that are out of reach of heuristic algorithms. It has also been successfully applied in wireless communications, even though the current radio systems are well-understood and optimal algorithms exist for many tasks. While some gains have been obtained by learning individual parts of a receiver, a better approach is to jointly learn the whole receiver. This, however, often results in a challenging nonlinear problem, for which the optimal solution is infeasible to implement. To this end, we propose a deep fully convolutional neural network, deeprx, which executes the whole receiver pipeline from frequency domain signal stream to uncoded bits in a 5g-compliant fashion. We facilitate accurate channel estimation by constructing the input of the convolutional neural network in a very specific manner using both the data and pilot symbols."
"Mutation-based fuzzing typically uses an initial set of non-crashing seed inputs (a corpus) from which to generate new inputs by mutation. A corpus of potential seeds will often contain thousands of similar inputs. This lack of diversity can lead to wasted fuzzing effort by exhaustive mutation from all available seeds. To address this, fuzzers come with distillation tools (e.g., afl-cmin) that select the smallest subset of seeds that triggers the same range of instrumentation data points as the full corpus. Common practice suggests that minimizing the number and cumulative size of the seeds leads to more efficient fuzzing, which we explore systematically. We present results of 34+ cpu-years of fuzzing with five distillation approaches to understand their impact in finding bugs in real-world software."
"In this paper we investigate multi-agent discrete-event systems with partial observation. The agents can be divided into several groups in each of which the agents have similar (isomorphic) state transition structures, and thus can be relabeled into the same template. Based on the template a scalable supervisor whose state size and computational cost are independent of the number of agents is designed for the case of partial observation. The scalable supervisor under partial observation does not need to be recomputed regardless of how many agents are added to or removed from the system. We generalize our earlier results to partial observation by proposing sufficient conditions for safety and maximal permissiveness of the scalable least restrictive supervisor on the template level. An example is provided to illustrate the proposed scalable supervisory synthesis."
"Electric network frequency (enf) fluctuations constitute a powerful tool in multimedia forensics. An efficient approach for enf estimation is introduced with temporal windowing based on the filter-bank capon spectral estimator. A type of gohberg-semencul factorization of the model covariance matrix is used due to the toeplitz structure of the covariance matrix. Moreover, this approach uses, for the first time in the field of enf, a temporal window, not necessarily the rectangular one, at the stage preceding spectral estimation. Krylov matrices are employed for fast implementation of matrix inversions. The proposed approach outperforms the state-of-the-art methods in enf estimation, when a short time window of $1$ second is employed in power recordings. In speech recordings, the proposed approach yields highly accurate results with respect to both time complexity and accuracy."
"The direpack package aims to establish a set of modern statistical dimension reduction techniques into the python universe as a single, consistent package. The dimension reduction methods included resort into three categories: projection pursuit based dimension reduction, sufficient dimension reduction, and robust m estimators for dimension reduction. As a corollary, regularized regression estimators based on these reduced dimension spaces are provided as well, ranging from classical principal component regression up to sparse partial robust m regression. The package also contains a set of classical and robust pre-processing utilities, including generalized spatial signs, as well as dedicated plotting functionality and cross-validation utilities. Finally, direpack has been written consistent with the scikit-learn api, such that the estimators can flawlessly be included into (statistical and/or machine) learning pipelines in that framework."
"The nonlinear fluorescence emission has been widely applied for the high spatial resolution optical imaging. Here, we studied the fluorescence anomalous saturating effect of the nitrogen vacancy defect in diamond. The fluorescence reduction was observed with high power laser excitation. It increased the nonlinearity of the fluorescence emission, and changed the spatial frequency distribution of the fluorescence image. We used a differential excitation protocol to extract the high spatial frequency information. By modulating the excitation laser's power, the spatial resolution of imaging was improved approximate 1.6 times in comparison with the confocal microscopy. Due to the simplicity of the experimental setup and data processing, we expect this method can be used for improving the spatial resolution of sensing and biological labeling with the defects in solids."
"When random effects are correlated with sample design variables, the usual approach of employing individual survey weights (constructed to be inversely proportional to the unit survey inclusion probabilities) to form a pseudo-likelihood no longer produces asymptotically unbiased inference. We construct a weight-exponentiated formulation for the random effects distribution that achieves unbiased inference for generating hyperparameters of the random effects. We contrast our approach with frequentist methods that rely on numerical integration to reveal that only the bayesian method achieves both unbiased estimation with respect to the sampling design distribution and consistency with respect to the population generating distribution. Our simulations and real data example for a survey of business establishments demonstrate the utility of our approach across different modeling formulations and sampling designs. This work serves as a capstone for recent developmental efforts that combine traditional survey estimation approaches with the bayesian modeling paradigm and provides a bridge across the two rich but disparate sub-fields."
"Low-symmetry 2d materials---such as res$_2$ and rese$_2$ monolayers, black phosphorus monolayers, group-iv monochalcogenide monolayers, borophene, among others---have more complex atomistic structures than the honeycomb lattices of graphene, hexagonal boron nitride, and transition metal dichalcogenides. The reduced symmetries of these emerging materials give rise to inhomogeneous electron, optical, valley, and spin responses, as well as entirely new properties such as ferroelasticity, ferroelectricity, magnetism, spin-wave phenomena, large nonlinear optical properties, photogalvanic effects, and superconductivity. Novel electronic topological properties, nonlinear elastic properties, and structural phase transformations can also take place due to low symmetry. The ""beyond graphene: low-symmetry and anisotropic 2d materials"" special topic was assembled to highlight recent experimental and theoretical research on these emerging materials."
"Orientational dynamics in the isotropic phase of a comb-shaped nematic polymer with mesogenic and functional side groups was studied using the kerr effect and dielectric spectroscopy. For the first time, it was found that in a mesogenic polymer, in contrast to low-molecular-weight mesogens, the relaxation of the electric birefringence of a melt above the temperature of the nematic-isotropic phase transition can be presented by a sum of several exponential processes, two of which play a decisive role. These main processes replace each other in a temperature range of about fifty degrees. Dielectric spectroscopy also made it possible to distinguish two processes of orientational relaxation: the first is due to rotation of the side mesogenic groups, and the second is associated with motion of the main chain segments."
"Deep hashing methods have shown great retrieval accuracy and efficiency in large-scale image retrieval. How to optimize discrete hash bits is always the focus in deep hashing methods. A common strategy in these methods is to adopt an activation function, e.g. $\operatorname{sigmoid}(\cdot)$ or $\operatorname{tanh}(\cdot)$, and minimize a quantization loss to approximate discrete values. However, this paradigm may make more and more hash bits stuck into the wrong saturated area of the activation functions and never escaped. We call this problem ""dead bits problem~(dbp)"". Besides, the existing quantization loss will aggravate dbp as well. In this paper, we propose a simple but effective gradient amplifier which acts before activation functions to alleviate dbp."
"We study the phenomenology of a hypercharge-zero su(2) triplet scalar whose existence is motivated by two-step electroweak symmetry-breaking. We consider both the possibility that the triplets are stable and contribute to the dark matter density, or that they decay via mixing with the standard model higgs boson. The former is constrained by disappearing charged track searches at the lhc and by dark matter direct detection experiments, while the latter is constrained by existing multilepton collider searches. We find that a two-step electroweak phase transition involving a stable triplet with a negative quadratic term is ruled out by direct detection searches, while an unstable triplet with a mass less than $230\ \mathrm{gev}$ is excluded at $95\%$ confidence level."
"This paper shows that the eccentric debris rings seen around the stars fomalhaut and hd 202628 are narrower than expected in the standard eccentric planet perturbation scenario (sometimes referred to as ""pericenter glow""). The standard scenario posits an initially circular and narrow belt of planetesimals at semi-major axis $a$, whose eccentricity is increased to $e_f$ after the gas disc has dispersed by secular perturbations from an eccentric planet, resulting in a belt of width $2ae_f$. In a minor modification of this scenario, narrower belts can arise if the planetesimals are initially eccentric, which could result from earlier planet perturbations during the gas-rich protoplanetary disc phase. However, a primordial eccentricity could alternatively be caused by instabilities that increase the disc eccentricity, without the need for any planets."
"An electron is usually considered to have only one type of kinetic energy, but could it have more, for its spin and charge, or by exciting other electrons? in one dimension (1d), the physics of interacting electrons is captured well at low energies by the tomonaga-luttinger-liquid (tll) model, yet little has been observed experimentally beyond this linear regime. Here, we report on measurements of many-body modes in 1d gated-wires using a tunnelling spectroscopy technique. We observe two separate fermi seas at high energies, associated with spin and charge excitations, together with the emergence of three additional 1d 'replica' modes that strengthen with decreasing wire length. The effective interaction strength in the wires is varied by changing the amount of 1d inter-subband screening by over 45%."
"$context$. The assembly history experienced by the milky way is currently being unveiled thanks to the data provided by the $gaia$ mission. It is likely that the globular cluster system of our galaxy has followed a similarly intricate formation path. $aims$. To constrain this formation path, we explore the link between the globular clusters and the known merging events that the milky way has experienced. $methods$. To this end, we combined the kinematic information provided by $gaia$ for almost all galactic clusters, with the largest sample of cluster ages available after carefully correcting for systematic errors. To identify clusters with a common origin we analysed their dynamical properties, particularly in the space of integrals of motion. $results$. We find that about 40% of the clusters likely formed in situ."
"The estimation of the intrinsic dimension of a dataset is a fundamental step in most dimensionality reduction techniques. This article illustrates intrinsic, an r package that implements novel state-of-the-art likelihood-based estimators of the intrinsic dimension of a dataset. In detail, the methods included in this package are the two-nn, gride, and hidalgo models. To allow these novel estimators to be easily accessible, the package contains a few high-level, intuitive functions that rely on a broader set of efficient, low-level routines. Intrinsic encompasses models that fall into two categories: homogeneous and heterogeneous intrinsic dimension estimators. The first category contains the two-nn and gride models. The functions dedicated to these two methods carry out inference under both the frequentist and bayesian frameworks. In the second category we find hidalgo, a bayesian mixture model, for which an efficient gibbs sampler is implemented."
"Spintronic nanodevices have ultrafast nonlinear dynamic and recurrence behaviors on a nanosecond scale that promises to enable spintronic reservoir computing (rc) system. Here two physical rc systems based on a single magnetic skyrmion memristor (msm) and 24 spin-torque nano-oscillators (stnos) were proposed and modeled to process image classification task and nonlinear dynamic system prediction, respectively. Based on our micromagnetic simulation results on the nonlinear responses of msm and stno with current pulses stimulation, the handwritten digits recognition task domesticates that an rc system using one single msm has the outstanding performance on image classification. In addition, the complex unknown nonlinear dynamic problems can also be well solved by a physical rc system consisted of 24 stnos confirmed in a second-order nonlinear dynamic system and narma10 tasks."
"Ranking algorithms play a crucial role in online platforms ranging from search engines to recommender systems. In this paper, we identify a surprising consequence of popularity-based rankings: the fewer the items reporting a given signal, the higher the share of the overall traffic they collectively attract. This few-get-richer effect emerges in settings where there are few distinct classes of items (e.g., left-leaning news sources versus right-leaning news sources), and items are ranked based on their popularity. We demonstrate analytically that the few-get-richer effect emerges when people tend to click on top-ranked items and have heterogeneous preferences for the classes of items. Using simulations, we analyze how the strength of the effect changes with assumptions about the setting and human behavior. We also test our predictions experimentally in an online experiment with human participants. Our findings have important implications to understand the spread of misinformation."
"Weight pruning has been widely acknowledged as a straightforward and effective method to eliminate redundancy in deep neural networks (dnn), thereby achieving acceleration on various platforms. However, most of the pruning techniques are essentially trade-offs between model accuracy and regularity which lead to impaired inference accuracy and limited on-device acceleration performance. To solve the problem, we introduce a new sparsity dimension, namely pattern-based sparsity that comprises pattern and connectivity sparsity, and becoming both highly accurate and hardware friendly. With carefully designed patterns, the proposed pruning unprecedentedly and consistently achieves accuracy enhancement and better feature extraction ability on different dnn structures and datasets, and our pattern-aware pruning framework also achieves pattern library extraction, pattern selection, pattern and connectivity pruning and weight training simultaneously. Our approach on the new pattern-based sparsity naturally fits into compiler optimization for highly efficient dnn execution on mobile platforms."
"Difference-in-differences analysis with a control group that differs considerably from a treated group is vulnerable to bias from historical events that have different effects on the groups. Constructing a more closely matched control group by matching a subset of the overall control group to the treated group may result in less bias. We study this phenomenon in simulation studies. We study the effect of mountaintop removal mining (mrm) on mortality using a difference-in-differences analysis that makes use of the increase in mrm following the 1990 clean air act amendments. For a difference-in-differences analysis of the effect of mrm on mortality, we constructed a more closely matched control group and found a 95\% confidence interval that contains substantial adverse effects along with no effect and small beneficial effects."
"When constructing models to summarize clinical data to be used for simulations, it is good practice to evaluate the models for their capacity to reproduce the data. This can be done by means of visual predictive checks (vpc), which consist of (1) several reproductions of the original study by simulation from the model under evaluation, (2) calculating estimates of interest for each simulated study and (3) comparing the distribution of those estimates with the estimate from the original study. This procedure is a generic method that is straightforward to apply, in general. Here we consider the application of the method to time to event data and consider the special case when a time-varying covariate is not known or cannot be approximated after event time."
"Novel concepts, perspectives and challenges in measuring and controlling an open quantum system via sequential schemes are shown. We discuss how similar protocols, relying both on repeated quantum measurements and dynamical decoupling control pulses, can allow to: (i) confine and protect quantum dynamics from decoherence in accordance with the zeno physics. (ii) analytically predict the probability that a quantum system is transferred into a target quantum state by means of stochastic sequential measurements. (iii) optimally reconstruct the spectral density of environmental noise sources by orthogonalizing in the frequency domain the filter functions driving the designed quantum-sensor. The achievement of these tasks will enhance our capability to observe and manipulate open quantum systems, thus bringing advances to quantum science and technologies."
"This paper presents a millimeter-scale cmos 64$\times$64 single charged particle radiation detector system for external beam cancer radiotherapy. A 1$\times$1 $\mu m^2$ diode measures energy deposition by a single charged particle in the depletion region, and the array design provides a large detection area of 512$\times$512 $\mu m^2$. Instead of sensing the voltage drop caused by radiation, the proposed system measures the pulse width, i.e., the time it takes for the voltage to return to its baseline. This obviates the need for using power-hungry and large analog-to-digital converters. A prototype asic is fabricated in tsmc 65 nm lp cmos process and consumes the average static power of 0.535 mw under 1.2 v analog and digital power supply. The functionality of the whole system is successfully verified in a clinical 67.5 mev proton beam setting."
